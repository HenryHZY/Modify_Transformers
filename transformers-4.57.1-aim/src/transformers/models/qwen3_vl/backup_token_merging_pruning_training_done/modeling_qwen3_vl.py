#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/qwen3_vl/modular_qwen3_vl.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3_vl.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union, Callable

import torch
import torch.nn as nn
import torch.nn.functional as F

from ...activations import ACT2FN
from ...cache_utils import Cache, DynamicCache
from ...generation import GenerationMixin
from ...integrations import use_kernel_forward_from_hub
from ...masking_utils import create_causal_mask
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput
from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling
from ...utils.deprecation import deprecate_kwarg
from ...utils.generic import check_model_inputs
from .configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLTextConfig, Qwen3VLVisionConfig
import einops
import copy

class Qwen3VLVisionMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.linear_fc1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=True)
        self.linear_fc2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=True)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, hidden_state):
        return self.linear_fc2(self.act_fn(self.linear_fc1(hidden_state)))


class Qwen3VLVisionPatchEmbed(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        self.patch_size = config.patch_size
        self.temporal_patch_size = config.temporal_patch_size
        self.in_channels = config.in_channels
        self.embed_dim = config.hidden_size

        kernel_size = [self.temporal_patch_size, self.patch_size, self.patch_size]
        self.proj = nn.Conv3d(self.in_channels, self.embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=True)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        target_dtype = self.proj.weight.dtype
        hidden_states = hidden_states.view(
            -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size
        )
        hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
        return hidden_states


class Qwen3VLVisionRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, dim: int, theta: float = 10000.0) -> None:
        super().__init__()
        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

    def forward(self, seqlen: int) -> torch.Tensor:
        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
        freqs = torch.outer(seq, self.inv_freq)
        return freqs


class Qwen3VLVisionPatchMerger(nn.Module):
    def __init__(self, config: Qwen3VLVisionConfig, use_postshuffle_norm=False) -> None:
        super().__init__()
        self.hidden_size = config.hidden_size * (config.spatial_merge_size**2)
        self.use_postshuffle_norm = use_postshuffle_norm
        self.norm = nn.LayerNorm(self.hidden_size if use_postshuffle_norm else config.hidden_size, eps=1e-6)
        self.linear_fc1 = nn.Linear(self.hidden_size, self.hidden_size)
        self.act_fn = nn.GELU()
        self.linear_fc2 = nn.Linear(self.hidden_size, config.out_hidden_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.norm(x.view(-1, self.hidden_size) if self.use_postshuffle_norm else x).view(-1, self.hidden_size)
        x = self.linear_fc2(self.act_fn(self.linear_fc1(x)))
        return x


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb_vision(
    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor
) -> tuple[torch.Tensor, torch.Tensor]:
    orig_q_dtype = q.dtype
    orig_k_dtype = k.dtype
    q, k = q.float(), k.float()
    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    q_embed = q_embed.to(orig_q_dtype)
    k_embed = k_embed.to(orig_k_dtype)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward_original(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor], 
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        # import pdb; pdb.set_trace()
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights

def eager_attention_forward_with_pruning(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor], # NOTE-ZY: input attention_mask created by create_causal_mask()
    scaling: float,
    dropout: float = 0.0,
    past_key_values: Optional[Cache] = None,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None: 
        # import pdb; pdb.set_trace()
        # attention_mask.shape = torch.Size([1, 1, 330, 330])
        # key_states.shape = torch.Size([1, 32, 330, 128])
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]] # torch.Size([1, 1, 330, 330]) == attention_mask
        attn_weights = attn_weights + causal_mask

    importance_score = None
    ############# token pruning #############
    q_len = query.shape[-2] # NOTE-ZY: query.shape torch.Size([1, 32, 330, 128]) -> (bs, num_heads, q_len, head_dim)
    importance_score = None
    is_prefill = past_key_values is None or past_key_values.get_seq_length() == 0
    flag = attention_mask.shape[-2] == attention_mask.shape[-1] # during standard prefill, attention_mask has shape (B, H, N, N)
    if q_len != 1 and flag: # prefill phrase
        importance_score = get_importance(attn_weights)
        # importance_score = torch.rand((attn_weights.shape[0], attn_weights.shape[3]), dtype=attn_weights.dtype, device=attn_weights.device)
    else:
        if is_prefill: # RL stage get_per_token_logps()
            importance_score = get_importance(attn_weights)
            # importance_score = torch.rand((attn_weights.shape[0], attn_weights.shape[3]), dtype=attn_weights.dtype, device=attn_weights.device)
        else: # decoding phrase
            importance_score = 'decoding'
    ############# token pruning #############


    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights, importance_score



class Qwen3VLVisionAttention(nn.Module):
    def __init__(self, config: Qwen3VLVisionConfig) -> None:
        super().__init__()
        self.dim = config.hidden_size
        self.num_heads = config.num_heads
        self.head_dim = self.dim // self.num_heads
        self.num_key_value_groups = 1  # needed for eager attention
        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)
        self.proj = nn.Linear(self.dim, self.dim)
        self.scaling = self.head_dim**-0.5
        self.config = config
        self.attention_dropout = 0.0
        self.is_causal = False

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> torch.Tensor:
        seq_length = hidden_states.shape[0]
        query_states, key_states, value_states = (
            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)
        )
        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)

        query_states = query_states.transpose(0, 1).unsqueeze(0)
        key_states = key_states.transpose(0, 1).unsqueeze(0)
        value_states = value_states.transpose(0, 1).unsqueeze(0)

        attention_interface: Callable = eager_attention_forward_original
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        if self.config._attn_implementation == "flash_attention_2":
            # Flash Attention 2: Use cu_seqlens for variable length attention
            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()
            attn_output, _ = attention_interface(
                self,
                query_states,
                key_states,
                value_states,
                attention_mask=None,
                scaling=self.scaling,
                dropout=0.0 if not self.training else self.attention_dropout,
                cu_seq_lens_q=cu_seqlens,
                cu_seq_lens_k=cu_seqlens,
                max_length_q=max_seqlen,
                max_length_k=max_seqlen,
                is_causal=False,
                **kwargs,
            )
        else:
            # Other implementations: Process each chunk separately
            lengths = cu_seqlens[1:] - cu_seqlens[:-1]
            splits = [
                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)
            ]

            attn_outputs = [
                attention_interface(
                    self,
                    q,
                    k,
                    v,
                    attention_mask=None,
                    scaling=self.scaling,
                    dropout=0.0 if not self.training else self.attention_dropout,
                    is_causal=False,
                    **kwargs,
                )[0]
                for q, k, v in zip(*splits)
            ]
            attn_output = torch.cat(attn_outputs, dim=1)

        attn_output = attn_output.reshape(seq_length, -1).contiguous()
        attn_output = self.proj(attn_output)
        return attn_output


class Qwen3VLVisionBlock(GradientCheckpointingLayer):
    def __init__(self, config, attn_implementation: str = "sdpa") -> None:
        super().__init__()
        self.norm1 = nn.LayerNorm(config.hidden_size, eps=1e-6)
        self.norm2 = nn.LayerNorm(config.hidden_size, eps=1e-6)
        self.attn = Qwen3VLVisionAttention(config=config)
        self.mlp = Qwen3VLVisionMLP(config=config)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> torch.Tensor:
        hidden_states = hidden_states + self.attn(
            self.norm1(hidden_states),
            cu_seqlens=cu_seqlens,
            rotary_pos_emb=rotary_pos_emb,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))
        return hidden_states


class Qwen3VLTextRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Qwen3VLTextConfig, device=None):
        super().__init__()
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", "default")
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

        self.mrope_section = config.rope_scaling.get("mrope_section", [24, 20, 20])

    def apply_interleaved_mrope(self, freqs, mrope_section):
        """Apply interleaved MRoPE to 3D rotary embeddings.
        Reorganizes frequency layout from chunked [TTT...HHH...WWW] to
        interleaved [THTHWHTHW...TT], preserving frequency continuity.
        args:
            x: (3, bs, seq_len, head_dim // 2)
            mrope_section: (3,)
        returns:
            x_t: (bs, seq_len, head_dim // 2)
        """
        freqs_t = freqs[0]  # just overwrite the first dimension T
        for dim, offset in enumerate((1, 2), start=1):  # H, W
            length = mrope_section[dim] * 3
            idx = slice(offset, length, 3)
            freqs_t[..., idx] = freqs[dim, ..., idx]
        return freqs_t

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        # In contrast to other models, Qwen3VL has different position ids for the grids
        # So we expand the inv_freq to shape (3, ...)
        if position_ids.ndim == 2:
            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)
        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)
        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)
            freqs = self.apply_interleaved_mrope(freqs, self.mrope_section)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


@use_kernel_forward_from_hub("RMSNorm")
class Qwen3VLTextRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps: float = 1e-6) -> None:
        """
        Qwen3VLTextRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class Qwen3VLTextAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.q_norm = Qwen3VLTextRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
        self.k_norm = Qwen3VLTextRMSNorm(
            self.head_dim, eps=config.rms_norm_eps
        )  # thus post q_norm does not need reshape

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_values is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward_with_pruning
        if self.config._attn_implementation != "eager": # NOTE-ZY: e.g., "flash_attention_2" 
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
        
        # attn_output, attn_weights = attention_interface()
        # attn_output, attn_weights, importance_score = attention_interface()
        outputs = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            past_key_values=past_key_values,
            **kwargs,
        )
        attn_output = outputs[0]
        attn_weights = outputs[1]
        importance_score = outputs[2] if len(outputs) > 2 else None

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights, importance_score


class Qwen3VLTextMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class Qwen3VLTextDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen3VLTextConfig, layer_idx: int, do_pruning=False):
        super().__init__()
        self.hidden_size = config.hidden_size
        
        self.layer_idx_temp = layer_idx
        
        # self.self_attn = Qwen3VLTextAttention(config=config, layer_idx=layer_idx)
        if not do_pruning: # NOTE-ZY: normally, use Qwen3VLTextAttention with attn_implementation="flash_attention_2"
            # import pdb; pdb.set_trace()
            config_temp = copy.deepcopy(config)
            assert config_temp._attn_implementation == "flash_attention_2"
            self.self_attn = Qwen3VLTextAttention(config=config_temp, layer_idx=layer_idx)
            self.self_attn_type = "flash_attention_2"
        else:
            # NOTE-ZY: use eager attention with pruning
            # import pdb; pdb.set_trace()
            config_temp = copy.deepcopy(config)
            assert config_temp._attn_implementation == "eager"
            self.self_attn = Qwen3VLTextAttention(config=config_temp, layer_idx=layer_idx)
            self.self_attn_type = "eager"

        self.mlp = Qwen3VLTextMLP(config)
        self.input_layernorm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> torch.Tensor:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        # Self Attention
        
        # eager: hidden_states, _, importance_score = self.self_attn
        # others: hidden_states, _, _ = self.self_attn
        # NOTE-ZY: 
        # åœ¨is_prefillé˜¶æ®µï¼š
        #   attention_mask = None for flash attn; 
        #   attention_mask = torch.Size([1, 1, 330, 330]) for "eager"
        #   attention_mask = None for flash attn + token pruningã€é»˜è®¤æ˜¯è¿™ä¸ªï¼Œdebugä¹Ÿæ˜¯ä¸ºäº†è¿™ä¸ªã€‘
        outputs = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask, 
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        # if outputs[2] != None: # NOTE-ZY: æ­£å¸¸æƒ…å†µä¸‹ï¼Œä½¿ç”¨attn_implementation="flash_attention_2"æ—¶ï¼Œimportance_score!=Noneå¯¹åº” layer_idx wit pruningï¼Œä¹Ÿå°±æ˜¯Do pruning: layer 18,19,20,21,22
        hidden_states = outputs[0] # torch.Size([1, 330, 4096])
        importance_score = outputs[2] if len(outputs) > 2 else None # outputs[2] = None or torch.Size([1, 330])
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        
        outputs = (hidden_states,)
        outputs += (importance_score, )
        
        return outputs


@dataclass
@auto_docstring(
    custom_intro="""
    Base class for Llava outputs, with hidden states and attentions.
    """
)
class Qwen3VLModelOutputWithPast(ModelOutput):
    r"""
    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
        `past_key_values` input) to speed up sequential decoding.
    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
        The rope index difference between sequence length and multimodal rope.
    """

    last_hidden_state: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None
    run_our_forward: Optional[bool] = None
    is_prefill: Optional[bool] = None
    rl_forward: Optional[bool] = None
    initial_boundaries: Optional[list[torch.LongTensor]] = None
    final_boundaries: Optional[list[torch.LongTensor]] = None
    input_ids: Optional[torch.LongTensor] = None

@auto_docstring
class Qwen3VLPreTrainedModel(PreTrainedModel):
    config: Qwen3VLConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen3VLTextDecoderLayer", "Qwen3VLVisionBlock"]
    _skip_keys_device_placement = "past_key_values"
    _supports_flash_attn = True
    _supports_sdpa = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": Qwen3VLTextDecoderLayer,
        "attentions": Qwen3VLTextAttention,
    }


class Qwen3VLVisionModel(Qwen3VLPreTrainedModel):
    config: Qwen3VLVisionConfig
    _no_split_modules = ["Qwen3VLVisionBlock"]

    def __init__(self, config, *inputs, **kwargs) -> None:
        super().__init__(config, *inputs, **kwargs)
        self.spatial_merge_size = config.spatial_merge_size
        self.patch_size = config.patch_size
        self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size

        self.patch_embed = Qwen3VLVisionPatchEmbed(
            config=config,
        )

        self.pos_embed = nn.Embedding(config.num_position_embeddings, config.hidden_size)
        self.num_grid_per_side = int(config.num_position_embeddings**0.5)

        head_dim = config.hidden_size // config.num_heads
        self.rotary_pos_emb = Qwen3VLVisionRotaryEmbedding(head_dim // 2)

        self.blocks = nn.ModuleList([Qwen3VLVisionBlock(config) for _ in range(config.depth)])
        self.merger = Qwen3VLVisionPatchMerger(
            config=config,
            use_postshuffle_norm=False,
        )

        self.deepstack_visual_indexes = config.deepstack_visual_indexes # NOTE-ZY: [8, 16, 24] for qwen3vl8B
        self.deepstack_merger_list = nn.ModuleList(
            [
                Qwen3VLVisionPatchMerger(
                    config=config,
                    use_postshuffle_norm=True,
                )
                for _ in range(len(config.deepstack_visual_indexes))
            ]
        )

        self.gradient_checkpointing = False

    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
        merge_size = self.spatial_merge_size

        max_hw = int(grid_thw[:, 1:].max().item())
        freq_table = self.rotary_pos_emb(max_hw)  # (max_hw, dim // 2)
        device = freq_table.device

        total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())
        pos_ids = torch.empty((total_tokens, 2), dtype=torch.long, device=device)

        offset = 0
        for num_frames, height, width in grid_thw:
            merged_h, merged_w = height // merge_size, width // merge_size

            block_rows = torch.arange(merged_h, device=device)  # block row indices
            block_cols = torch.arange(merged_w, device=device)  # block col indices
            intra_row = torch.arange(merge_size, device=device)  # intra-block row offsets
            intra_col = torch.arange(merge_size, device=device)  # intra-block col offsets

            # Compute full-resolution positions
            row_idx = block_rows[:, None, None, None] * merge_size + intra_row[None, None, :, None]
            col_idx = block_cols[None, :, None, None] * merge_size + intra_col[None, None, None, :]

            row_idx = row_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)
            col_idx = col_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)

            coords = torch.stack((row_idx, col_idx), dim=-1)

            if num_frames > 1:
                coords = coords.repeat(num_frames, 1)

            num_tokens = coords.shape[0]
            pos_ids[offset : offset + num_tokens] = coords
            offset += num_tokens

        embeddings = freq_table[pos_ids]  # lookup rotary embeddings
        embeddings = embeddings.flatten(1)
        return embeddings

    def fast_pos_embed_interpolate(self, grid_thw):
        grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]

        idx_list = [[] for _ in range(4)]
        weight_list = [[] for _ in range(4)]

        for t, h, w in zip(grid_ts, grid_hs, grid_ws):
            h_idxs = torch.linspace(0, self.num_grid_per_side - 1, h)
            w_idxs = torch.linspace(0, self.num_grid_per_side - 1, w)

            h_idxs_floor = h_idxs.int()
            w_idxs_floor = w_idxs.int()
            h_idxs_ceil = (h_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)
            w_idxs_ceil = (w_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)

            dh = h_idxs - h_idxs_floor
            dw = w_idxs - w_idxs_floor

            base_h = h_idxs_floor * self.num_grid_per_side
            base_h_ceil = h_idxs_ceil * self.num_grid_per_side

            indices = [
                (base_h[None].T + w_idxs_floor[None]).flatten(),
                (base_h[None].T + w_idxs_ceil[None]).flatten(),
                (base_h_ceil[None].T + w_idxs_floor[None]).flatten(),
                (base_h_ceil[None].T + w_idxs_ceil[None]).flatten(),
            ]

            weights = [
                ((1 - dh)[None].T * (1 - dw)[None]).flatten(),
                ((1 - dh)[None].T * dw[None]).flatten(),
                (dh[None].T * (1 - dw)[None]).flatten(),
                (dh[None].T * dw[None]).flatten(),
            ]

            for i in range(4):
                idx_list[i].extend(indices[i].tolist())
                weight_list[i].extend(weights[i].tolist())

        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)
        weight_tensor = torch.tensor(
            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device
        )
        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]
        patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]

        patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])

        patch_pos_embeds_permute = []
        merge_size = self.config.spatial_merge_size
        for pos_embed, t, h, w in zip(patch_pos_embeds, grid_ts, grid_hs, grid_ws):
            pos_embed = pos_embed.repeat(t, 1)
            pos_embed = (
                pos_embed.view(t, h // merge_size, merge_size, w // merge_size, merge_size, -1)
                .permute(0, 1, 3, 2, 4, 5)
                .flatten(0, 4)
            )
            patch_pos_embeds_permute.append(pos_embed)
        patch_pos_embeds = torch.cat(patch_pos_embeds_permute)
        return patch_pos_embeds

    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Args:
            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):
                The final hidden states of the model.
            grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):
                The temporal, height and width of feature shape of each image in LLM.

        Returns:
            `torch.Tensor`: hidden_states.
        """
        hidden_states = self.patch_embed(hidden_states)

        pos_embeds = self.fast_pos_embed_interpolate(grid_thw)
        hidden_states = hidden_states + pos_embeds

        rotary_pos_emb = self.rot_pos_emb(grid_thw)

        seq_len, _ = hidden_states.size()
        hidden_states = hidden_states.reshape(seq_len, -1)
        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
        position_embeddings = (emb.cos(), emb.sin())

        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(
            dim=0,
            # Select dtype based on the following factors:
            #  - FA2 requires that cu_seqlens_q must have dtype int32
            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw
            # See https://github.com/huggingface/transformers/pull/34852 for more information
            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
        )
        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)

        deepstack_feature_lists = []
        for layer_num, blk in enumerate(self.blocks):
            hidden_states = blk(
                hidden_states,
                cu_seqlens=cu_seqlens,
                position_embeddings=position_embeddings,
                **kwargs,
            )
            if layer_num in self.deepstack_visual_indexes: # NOTE-ZY: [8, 16, 24] for qwen3vl8B
                deepstack_feature = self.deepstack_merger_list[self.deepstack_visual_indexes.index(layer_num)](
                    hidden_states
                )
                deepstack_feature_lists.append(deepstack_feature)

        hidden_states = self.merger(hidden_states)

        return hidden_states, deepstack_feature_lists


@auto_docstring(
    custom_intro=(
        "Text part of Qwen3VL, "
        "not a pure text-only model, as DeepStack integrates visual features into the early hidden states."
    )
)
class Qwen3VLTextModel(Qwen3VLPreTrainedModel):
    config: Qwen3VLTextConfig
    _no_split_modules = ["Qwen3VLTextDecoderLayer"]

    def __init__(self, config: Qwen3VLTextConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)

        ############# token pruning #############
        self.run_our_forward = True
        if not self.run_our_forward:
            self.layers = nn.ModuleList(
                [Qwen3VLTextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
            )
        else:
            # scheduler that can be customized
            start_l = int(config.num_hidden_layers // 2)
            self.retain_rate = [max(0, 1.0 - (layer_i - start_l + 1) * 0.2) if layer_i >= start_l else 1.0 for layer_i in range(config.num_hidden_layers)]
            # self.retain_rate = [max(0, 1.0 - (layer_i - 14 + 1) * 0.2) if layer_i >= 14 else 1.0 for layer_i in range(config.num_hidden_layers)] 
            # self.retain_rate = [max(0, 1.0 - (layer_i - 18 + 1) * 0.2) if layer_i >= 18 else 1.0 for layer_i in range(config.num_hidden_layers)] 
            # self.retain_rate = [max(0, 1.0 - (layer_i - 18 + 1) * 0.1) if layer_i >= 18 else 1.0 for layer_i in range(config.num_hidden_layers)] 
            # self.retain_rate = [max(0, 1.0 - (layer_i - 14 + 1) * 0.1) if layer_i >= 14 else 1.0 for layer_i in range(config.num_hidden_layers)] 
            # self.retain_rate = [max(0, 1.0 - (layer_i - 9 + 1) * 0.1) if layer_i >= 9 else 1.0 for layer_i in range(config.num_hidden_layers)] 
            # self.retain_rate = [max(0, 1.0 - (layer_i - 0 + 1) * 0.1) if layer_i >= 0 else 1.0 for layer_i in range(config.num_hidden_layers)] 
            self.layers = nn.ModuleList()
            # initialize the layers that perform token pruning to be eager mode
            full_ratio = 1.0
            for layer_idx in range(config.num_hidden_layers): # NOTE-ZY: qwen2.5vl7bçš„num_hidden_layers=28ï¼Œqwen3vlçš„num_hidden_layers=36
                if layer_idx == 0: # the first layer
                    if self.retain_rate[0] == full_ratio:
                        # print(f"No pruning: layer {layer_idx}")
                        self.layers.append(Qwen3VLTextDecoderLayer(config, layer_idx))
                    else:
                        # print(f"Is pruning: layer {layer_idx}")
                        config_temp = copy.deepcopy(config)
                        config_temp._attn_implementation = "eager"
                        self.layers.append(Qwen3VLTextDecoderLayer(config_temp, layer_idx, do_pruning=True))
                else: # other layers
                    if (self.retain_rate[layer_idx] - self.retain_rate[layer_idx - 1]) == 0:
                        # print(f"No pruning: layer {layer_idx}")
                        self.layers.append(Qwen3VLTextDecoderLayer(config, layer_idx))
                    else:
                        config_temp = copy.deepcopy(config)
                        config_temp._attn_implementation = "eager"
                        # print(f"Is pruning: layer {layer_idx}") # Do pruning: layer 18, layer 19, layer 20, layer 21, layer 22
                        self.layers.append(Qwen3VLTextDecoderLayer(config_temp, layer_idx, do_pruning=True))
        ############# token pruning #############
        
        self.norm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3VLTextRotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        # Initialize weights and apply final processing
        self.post_init()
        self.config = config

    @check_model_inputs
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        # args for deepstack
        visual_pos_masks: Optional[torch.Tensor] = None,
        deepstack_visual_embeds: Optional[list[torch.Tensor]] = None,
        boundaries: Optional[list[torch.LongTensor]] = None,
        vis_grid_thw: Optional[torch.LongTensor] = None,
        rope_deltas: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, BaseModelOutputWithPast]:
        
        # # NOTE-ZY: qwen2.5vl video-r1ç”¨çš„ self.model(input_ids=None) -> è€Œqwen3vlçš„self.language_model(input_ids=None)
        # outputs = self.language_model( # NOTE-ZY: å¯¹åº”video-r1 qwen2_5_vl_video_r1/modeling_qwen2_5_vl.pyçš„self.model()
        #     input_ids=None,
        #     position_ids=position_ids, # torch.Size([3, 1, 330]) # NOTE-ZY: å·²ç»æ˜¯token mergingçš„äº†
        #     attention_mask=attention_mask, # torch.Size([1, 330])ï¼Œå…¨ä¸º1
        #     past_key_values=past_key_values,
        #     inputs_embeds=inputs_embeds, # torch.Size([1, 330, 4096])
        #     cache_position=cache_position, # len=330, tensor([  0,   1,   2, ..., 328, 329], device='cuda:0')
        #     visual_pos_masks=visual_pos_masks, # torch.Size([1, 330])
        #     deepstack_visual_embeds=deepstack_visual_embeds, # len=3, deepstack_visual_embeds[0].shape = torch.Size([240, 4096]) # NOTE-ZYï¼šå·²ç»è¿›è¡Œäº†token selection
        #     boundaries=boundaries, # e.g., [[(10, 41), (49, 80), (89, 120), (129, 160), (169, 200), (209, 240), (249, 280), (289, 320)], 321, 330]
        #     vis_grid_thw=vis_grid_thw, # tensor([[ 8, 16, 30]], device='cuda:0')
        #     rope_deltas=self.rope_deltas.clone(), # tensor([[-120]], device='cuda:0')
        #     **kwargs, # {'use_cache': True}
        # )
        
        r"""
        visual_pos_masks (`torch.Tensor` of shape `(batch_size, seqlen)`, *optional*):
            The mask of the visual positions.
        deepstack_visual_embeds (`list[torch.Tensor]`, *optional*):
            The deepstack visual embeddings. The shape is (num_layers, visual_seqlen, embed_dim).
            The feature is extracted from the different visual encoder layers, and fed to the decoder
            hidden states. It's from the paper DeepStack(https://arxiv.org/abs/2406.04334).
        """
        # print(f"class Qwen3VLTextModel.forward()å¼€å¤´ -> input_ids.shape = {input_ids.shape}")
        
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        # torch.jit.trace() doesn't support cache objects in the output
        if use_cache and past_key_values is None and not torch.jit.is_tracing():
            past_key_values = DynamicCache(config=self.config)

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        # the hard coded `3` is for temporal, height and width.
        if position_ids is None:
            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)
        elif position_ids.ndim == 2:
            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)

        if position_ids.ndim == 3 and position_ids.shape[0] == 4:
            text_position_ids = position_ids[0]
            position_ids = position_ids[1:]
        else:
            text_position_ids = position_ids[0] # NOTE-ZY: position_ids.ndim == 3 and position_ids.shape[0] == 3, è¿›å…¥è¿™ä¸ªï¼Œå–å¾—temporal posç”¨äºcreate_causal_mask()
        
        # NOTE-ZY: 
        # æ­£å¸¸æƒ…å†µä¸‹ï¼Œè‹¥ä½¿ç”¨attn_implementation="flash_attention_2"ï¼š
            # åœ¨prefill stageæ—¶ï¼Œpast_key_valuesä¸ºç©º (past_key_values.get_seq_length() = 0), causal_mask = None
            # åœ¨decoding stageæ—¶ï¼Œpast_key_valueséç©º (past_key_values.get_seq_length() != 0), causal_mask = None
        # æ­£å¸¸æƒ…å†µä¸‹ï¼Œè‹¥ä½¿ç”¨attn_implementation="eager"ï¼š
            # åœ¨prefill stageæ—¶ï¼Œpast_key_valuesä¸ºç©º (past_key_values.get_seq_length() = 0), causal_mask != None -> torch.Size([1, 1, 330, 330])
            # åœ¨decoding stageæ—¶ï¼Œpast_key_valueséç©º (past_key_values.get_seq_length() != 0), causal_mask != None -> TODO: ï¼Ÿ
        # token pruning + ä½¿ç”¨attn_implementation="flash_attention_2"
            # æ­¤å¤„åˆšå¼€å§‹forwardè¿˜æ˜¯é»˜è®¤ç”¨attn_implementation="flash_attention_2"ï¼Œå› æ­¤æ­¤å¤„çš„causal_maskä¸€ç›´ä¸ºNone
        
        # NOTE-ZY: è¿™é‡Œä¼šè¦†ç›–ä¹‹å‰çš„ attention_maskï¼Œä½†åç»­ token pruningæ“ä½œä¸è¾“å…¥çš„attention_maskç›¸å…³éœ€è¦é‡æ–°ç”Ÿæˆcausal_mask -> æ­¤å¤„ç”¨æ–°å˜é‡å
        # NOTE-ZY: create_causal_mask()å‡½æ•°å‰åï¼Œæ‰€æœ‰è¾“å…¥éƒ½æ— å˜åŒ–
        # attention_mask = create_causal_mask(
        causal_mask = create_causal_mask(
            config=self.config,
            input_embeds=inputs_embeds, # torch.Size([1, 330, 4096])
            attention_mask=attention_mask, # torch.Size([1, 330])ï¼Œå…¨ä¸º1 # NOTE-ZY: checked: video-r1ä¹Ÿå¦‚æ­¤
            cache_position=cache_position, # torch.Size([330]), tensor([  0,   1,   2, ..., 329]
            past_key_values=past_key_values, 
            position_ids=text_position_ids, # torch.Size([1, 330]), text's rope # NOTE-ZY: create causal maskè¾“å…¥ä½¿ç”¨çš„æ˜¯torch.Size([1, 330])è¿™ç§ï¼Œè€Œä¸æ˜¯3Dçš„
        )
        
        is_prefill = past_key_values is None or past_key_values.get_seq_length() == 0
        if is_prefill: # prefill stage
            pass
            # print(f"at prefill stage")
            # import pdb; pdb.set_trace()
            # first pdb at prefill stage:
            # attention_mask.shape = torch.Size([1, 330])
            # inputs_embeds.shape = torch.Size([1, 330, 4096])
            # cache_position.shape = torch.Size([330]); tensor([  0,   1,   2,   3,   4, ..., 329])
            # past_key_values.get_seq_length() = 0
            # causal_mask = None
        else: # decoding stage
            pass
            # print(f"at decoding stage")
            # import pdb; pdb.set_trace()
            # first pdb at decoding stage: # ä¸video-r1ç›¸ç¬¦
            # attention_mask.shape = torch.Size([1, 331]) # å…¨ä¸º1çš„mask
            # inputs_embeds.shape = torch.Size([1, 1, 4096]) # ç›¸å¯¹äºprefillshapeæ”¹å˜ TODO: åªèƒ½çœ‹è§å½“å‰tokenï¼Ÿ
            # cache_position.shape = torch.Size([1]); tensor([  330]) # æ•°å€¼é€’å¢ï¼Œç›¸å¯¹äºprefillshapeæ”¹å˜
            # past_key_values.get_seq_length() = 330
            # causal_mask = None

            # seconds pdb at decoding stage:
            # attention_mask.shape = torch.Size([1, 332]) -> # TODO: shapeé€’å¢? å¥½åƒæœ‰ç‚¹é—®é¢˜ï¼Ÿ
            # inputs_embeds.shape = torch.Size([1, 1, 4096])
            # cache_position.shape = torch.Size([1]); tensor([  331]) # æ•°å€¼é€’å¢
            # past_key_values.get_seq_length() = 331 # æ•°å€¼é€’å¢
            # causal_mask = None            

        hidden_states = inputs_embeds # torch.Size([1, 330, 4096])

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids) # NOTE-ZY: æ­¤å¤„rope posæ˜¯mergingåçš„ï¼Œç„¶åå¾—åˆ°ç›¸åº”çš„pos emb
            # len(position_embeddings) = 2,
            # position_embeddings[0].shape = position_embeddings[1].shape = torch.Size([1, 330, 128]) å¯¹åº” cos, sin
        
        ############# token pruning #############
        is_prefill = past_key_values is None or past_key_values.get_seq_length() == 0 # denote prefill stage
        run_our_forward = self.run_our_forward # boundaries is not None
        layer_idx = 0
        # initialize key vairables across layers
        if is_prefill: # prefill phrase
            if run_our_forward:
                self.attention_mask_cache = [None for _ in range(len(self.layers) + 1)]
                self.attention_mask_cache[0] = attention_mask
                self.position_ids_cache = [None for _ in range(len(self.layers) + 1)]
                self.position_ids_cache[0] = position_ids
                self.cache_position_cache = [None for _ in range(len(self.layers) + 1)]
                self.cache_position_cache[0] = cache_position
                self.boundaries_cache = [None for _ in range(len(self.layers) + 1)]
                self.boundaries_cache[0] = boundaries
                    # self.boundaries_cache = [[[(10, 41), (49, 80), (89, 120), (129, 160), (169, 200), (209, 240), (249, 280), (289, 320)], 321, 330], None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]
                    # len(self.boundaries_cache) = 37
                    
                # protected_inds: all indices in a batch; protected_num: number of tokens per sample in a batch
                # hidden_states.shape = torch.Size([1, 330, 4096])
                
                protected_inds, protected_num = get_protected_info(boundaries, hidden_states) # NOTE-ZY: ä½œç”¨æ˜¯æ ‡è®°éè§†è§‰tokenä½ç½®ã€‚å‡½æ•°å®ç°éœ€è¦boundaryå®šä¹‰è°ƒæ•´ã€‚used in prefill stage, not used in decoing stage
                # print(f"protected_num = {protected_num}")
                # print(f"protected_inds.shape = {protected_inds.shape}")
                # protected_num = tensor([90], device='cuda:0')
                # protected_inds.shape = torch.Size([90, 2])
                # import pdb; pdb.set_trace()
                v_cnt_before_llm = hidden_states.size(1) - protected_num # 330 - 90 = tensor([240], device='cuda:0')
        else: # decoding phrase
            running_rope_deltas = rope_deltas.clone() # avoid in-place edits
        ############# token pruning #############

        # decoder layers
        for layer_idx, decoder_layer in enumerate(self.layers): # NOTE-ZY: LLMå†…çš„æ¯ä¸€å±‚ï¼Œéœ€è¦è€ƒè™‘prefill / decoding stageï¼Œæ˜¯å¦åˆ‡æ¢eager / flash attn
            # NOTE-ZY: checkäº†env video-r1ï¼Œé€»è¾‘å·®ä¸å¤š
            # if layer_idx == 17 or layer_idx == 18 or layer_idx == 19 or layer_idx == 22 or layer_idx == 23: # flash attn -> eager attn -> eager attn -> flash attn
                # print(f"layer_idx = {layer_idx}")
                # import pdb; pdb.set_trace()
                
                # pruning layer: 18, 19, 20, 21, 22
                # flash attn -> eager: 17 -> 18
                # eager -> flash attn: 22 -> 23
                # flash attn: å…¶ä»–ï¼Œæ¯”å¦‚17, 23
                
                # layer_idx = 17 without pruning, flash attn
                # hidden_states.shape = torch.Size([1, 330, 4096])
                # causal_mask = None
                # text_position_ids.shape = torch.Size([1, 330])
                # past_key_values.get_seq_length() = 330
                # position_embeddings[0].shape = torch.Size([1, 330, 128])
                
                # layer_idx = 18 with pruning, eager
                # hidden_states.shape = torch.Size([1, 330, 4096])
                # causal_mask.shape = torch.Size([1, 1, 330, 330])
                # text_position_ids.shape = torch.Size([1, 330])
                # past_key_values.get_seq_length() = 330
                # position_embeddings[0].shape = torch.Size([1, 330, 128])
                
                # layer_idx = 19 with pruning, eager
                # hidden_states.shape = torch.Size([1, 282, 4096])
                # causal_mask.shape = torch.Size([1, 1, 282, 282])
                # text_position_ids.shape = torch.Size([1, 282]) # NOTE-ZY: fix, è¿™ä¸ªupdate
                # past_key_values.get_seq_length() = 330
                # position_embeddings[0].shape = torch.Size([1, 282, 128])

                # layer_idx = 22 with pruning, eager
                # hidden_states.shape = torch.Size([1, 138, 4096])
                # causal_mask.shape = torch.Size([1, 1, 138, 138])
                # text_position_ids.shape = torch.Size([1, 138]) 
                # past_key_values.get_seq_length() = 330
                # position_embeddings[0].shape = torch.Size([1, 138, 128])
                
                # layer_idx = 23 without pruning, flash attn
                # hidden_states.shape = torch.Size([1, 90, 4096])
                # causal_mask.shape = None
                # text_position_ids.shape = torch.Size([1, 90]) 
                # past_key_values.get_seq_length() = 330
                # position_embeddings[0].shape = torch.Size([1, 90, 128])

            # layer_outputs <- hidden_states, importance_score
            layer_outputs = decoder_layer(
                hidden_states,
                # attention_mask=attention_mask, # NOTE-ZY: original code also pass attention_mask created by create_causal_mask(), i.e., causal_mask
                attention_mask=causal_mask,
                position_ids=text_position_ids, 
                past_key_values=past_key_values,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs,
            )
            
            
            hidden_states = layer_outputs[0] # torch.Size([1, 330, 4096])
            importance_score = layer_outputs[-1] 
            
            # add visual features to the hidden states of first several layers
            if deepstack_visual_embeds is not None and layer_idx in range(len(deepstack_visual_embeds)): 
                # NOTE-ZY: len([8,16,24])=3, layer_idx = 0, 1, 2 for qwen3vl8B
                # ç›¸å½“äºå‰ä¸‰å±‚hidden_statesçš„è§†è§‰ç‰¹å¾ï¼ˆé€šè¿‡visual_pos_masksæ‰¾åˆ°è§†è§‰ç‰¹å¾ï¼‰çš„æ®‹å·®è¿æ¥ -> hidden_states[visual_pos_masks, :] += deepstack_visual_embeds[layer_idx]
                # deepstack_visual_embeds[0].shape = torch.Size([240, 4096])
                # len(deepstack_visual_embeds) = 3
                # visual_pos_masks.shape = torch.Size([1, 330])
                hidden_states = self._deepstack_process(
                    hidden_states,
                    visual_pos_masks,
                    deepstack_visual_embeds[layer_idx],
                )

            ############# token pruning #############
            if run_our_forward: 
                # å¼€å§‹é€‰token, key tokens, score = attn_wï¼Œå…·ä½“è®¡ç®—æœ‰ç°æˆfunc 
                # generate, ç¬¬ä¸€æ¬¡forward prefillï¼Œç¬¬äºŒæ¬¡forward decode 
                # prefill: condition input -> kvcache 
                # decodeç¬¬ä¸€ä¸ª len(input)=1
                # decode: kvcache + cur -> next pred
                # will update kvcache -> attn.shape add 1, also for cache_pos
                # batchæ“ä½œï¼šæ¯å±‚å…ˆè¦å‡kä¸ªtoken -> æœ€åå†+1 (next pred) -> batch padding (ç”±äºæ¯è¡Œæ•°é‡ä¸å®šï¼Œä½†æ˜¯å®é™…ä¸Šä¸€èˆ¬éƒ½æ˜¯bs=1ï¼Œä¸è¿‡codeæœ‰æ”¯æŒbs>1)
                
                # å¯¹äºå¼ºåŒ–å­¦ä¹ ï¼Œgenerateä¹Ÿæ˜¯prefill + decodeï¼Œ
                # ç„¶åget per log at prefill stage
                
                if importance_score is None: 
                    # importance_score is None -> å½“å‰layeræ˜¯flash attentionçš„layer -> ä½†æ˜¯éœ€è¦åˆ†åˆ«åœ¨prefill å’Œ decoding stageæ—¶ï¼Œä¸ºä¸‹ä¸€å±‚æ˜¯eager attentionçš„layeråšè¡”æ¥ï¼Œç”Ÿæˆè¦æ±‚çš„causal attention
                    # ã€å¦‚æœæ˜¯çº¯eagerï¼Œä¼šOOMã€‘
                    if is_prefill: # prefill phrase or RL stage get_per_token_logps()
                        # print(f"at prefill stage: layer_idx = {layer_idx} without pruning")
                        self.attention_mask_cache[layer_idx + 1] = attention_mask
                        self.position_ids_cache[layer_idx + 1] = position_ids
                        self.boundaries_cache[layer_idx + 1] = boundaries
                        self.cache_position_cache[layer_idx + 1] = cache_position
                        
                        if (layer_idx + 1) < len(self.layers): # NOTE-ZY: æœ¬å±‚ä¸ä¸‹ä¸€å±‚çš„è¡”æ¥
                            if self.layers[layer_idx + 1].self_attn_type == 'eager': # layer transition: recompute mask for next layer (eager type)
                                # NOTE-ZY: prefill stageï¼Œä¸‹ä¸€å±‚æ˜¯eagerï¼Œä¸ºä¸‹ä¸€å±‚ç”Ÿæˆ4D causal_mask
                                    # å½“å‰layer_idx = 17æ˜¯flash attnï¼Œä¸‹ä¸€å±‚æ˜¯eager
                                    # causal_mask -> torch.Size([1, 1, 330, 330])
                                config_temp = copy.deepcopy(self.config)
                                config_temp._attn_implementation = "eager"
                                text_position_ids=position_ids[0]
                                assert hidden_states.shape[1] == text_position_ids.shape[1], f"hidden_states.shape[1] = {hidden_states.shape[1]} != text_position_ids.shape[1] = {text_position_ids.shape[1]}"
                                causal_mask_temp = create_causal_mask(config=config_temp, input_embeds=hidden_states, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=text_position_ids, compression_prefill=True, compression_decode=False)
                                causal_mask = causal_mask_temp
                                
                    else: # decoding phrase (append new token) # NOTE-ZY: å…ˆçœ‹prefill stageï¼Œå†çœ‹decoding stage
                        # get variables of next layer from prefill cache
                        attention_mask = self.attention_mask_cache[layer_idx + 1] # len=37ï¼Œç”¨1~37å…±36å±‚
                        position_ids = self.position_ids_cache[layer_idx + 1] 
                        cache_position = self.cache_position_cache[layer_idx + 1] 
                        boundaries = self.boundaries_cache[layer_idx + 1]

                        # convert variables to decoding stage (usually one input token)
                        batch_size, seq_length, _ = hidden_states.shape # torch.Size([1, 1, 4096]) TODO: decodingçš„hidden_stateså¦‚ä½•å¾—åˆ°ï¼Ÿ
                        attention_mask = torch.cat((attention_mask, attention_mask.new_full((attention_mask.shape[0], seq_length), fill_value=1)), dim=-1) 
                            # append mask for the input token
                            # input attention_mask.shape = torch.Size([1, 330]), å…¨ä¸º1
                            # output attention_mask.shape = torch.Size([1, 331]), å…¨ä¸º1
                        cache_position = torch.arange(seq_length, device=cache_position.device) + cache_position[-1].item() + 1 # cache position for input token
                            # tensor([330], device='cuda:0')
                            # NOTE: cache_position value would decrease as layer goes deeper, yet position_ids should keep increasing as if there was no token compression
                        reduct_cnt = self.boundaries_cache[layer_idx][1] - self.boundaries_cache[layer_idx + 1][1] # NOTE-ZY: ç›¸å½“äºq start idxç›¸å‡ç®—å‡ºreduct_cnt
                        running_rope_deltas += reduct_cnt
                            # running_rope_deltas = tensor([[-120]], device='cuda:0')
                        delta = ((cache_position[0] + running_rope_deltas).to(hidden_states.device) if cache_position is not None else 0)
                            # 330 + (-120) = 210
                            # delta = tensor([[210]], device='cuda:0')
                        position_ids = torch.arange(seq_length, device=hidden_states.device)
                        position_ids = position_ids.view(1, -1).expand(batch_size, -1)
                        if cache_position is not None:  # otherwise `deltas` is an int `0`
                            delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)
                        position_ids = position_ids.add(delta)
                        position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

                        # re-compute for next layer
                        if (layer_idx + 1) < len(self.layers):
                            if self.layers[layer_idx + 1].self_attn_type == 'eager': # layer transition: recompute mask for next layer (eager type)
                                config_temp = copy.deepcopy(self.config)
                                config_temp._attn_implementation = "eager"
                                text_position_ids=position_ids[0]
                                assert hidden_states.shape[1] == text_position_ids.shape[1], f"hidden_states.shape[1] = {hidden_states.shape[1]} != text_position_ids.shape[1] = {text_position_ids.shape[1]}"
                                causal_mask_temp = create_causal_mask(config=config_temp, input_embeds=hidden_states, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=text_position_ids, compression_prefill=False, compression_decode=True)
                                causal_mask = causal_mask_temp
                                
                        position_embeddings = self.rotary_emb(hidden_states, position_ids) # computed w.r.t. position_ids values, irrelevant to hidden_states values

                        # update cache for next token input
                        self.attention_mask_cache[layer_idx + 1] = attention_mask # [item.shape[1] for item in self.attention_mask_cache]
                        self.cache_position_cache[layer_idx + 1] = cache_position # [item.shape[2] for item in self.position_ids_cache], self.position_ids_cache[0], self.position_ids_cache[-1] 
                        self.position_ids_cache[layer_idx + 1] = position_ids # [item[-1] for item in self.cache_position_cache]
                        self.boundaries_cache[layer_idx + 1][2] += 1 # self.boundaries_cache, boundary at each layer keeps the same, except that the seq length + 1
                else: 
                    # importance_score is not None -> å½“å‰layeræ˜¯eager attentionçš„layer -> éœ€è¦åštoken pruningçš„æ“ä½œ
                    # åŒæ ·åˆ†ä¸ºprefill stageå’Œdecoding stage
                    # import pdb; pdb.set_trace()
                    if importance_score == 'decoding': # decoding phrase # NOTE-ZY: å…ˆçœ‹prefill stageï¼Œå†çœ‹decoding stage
                        # append new token # ä¸ºäº†ä¸€ä¸ªæ–°tokenï¼Œå¹¶ä¸”çœŸçš„ä¸¢æ‰
                        # get variables of next layer from prefill cache
                        # import pdb; pdb.set_trace()
                        attention_mask = self.attention_mask_cache[layer_idx + 1] 
                        position_ids = self.position_ids_cache[layer_idx + 1] 
                        cache_position = self.cache_position_cache[layer_idx + 1] 
                        boundaries = self.boundaries_cache[layer_idx + 1]

                        # convert variables to decoding stage (usually one input token)
                        batch_size, seq_length, _ = hidden_states.shape
                        attention_mask = torch.cat((attention_mask, attention_mask.new_full((attention_mask.shape[0], seq_length), fill_value=1)), dim=-1) # append mask for the input token
                        cache_position = torch.arange(seq_length, device=cache_position.device) + cache_position[-1].item() + 1 # cache position for input token
                        # NOTE: cache_position value would decrease as layer goes deeper, yet position_ids should keep increasing as if there was no token compression
                        reduct_cnt = self.boundaries_cache[layer_idx][1] - self.boundaries_cache[layer_idx + 1][1] # NOTE-ZY: ç›¸å½“äºq start idxç›¸å‡ç®—å‡ºreduct_cnt
                        running_rope_deltas += reduct_cnt
                        delta = ((cache_position[0] + running_rope_deltas).to(hidden_states.device) if cache_position is not None else 0)
                        position_ids = torch.arange(seq_length, device=hidden_states.device)
                        position_ids = position_ids.view(1, -1).expand(batch_size, -1)
                        if cache_position is not None:  # otherwise `deltas` is an int `0`
                            delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)
                        position_ids = position_ids.add(delta)
                        position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

                        # re-compute for next layer
                        if (layer_idx + 1) < len(self.layers):
                            if self.layers[layer_idx + 1].self_attn_type != 'eager': # layer transition: recompute mask for next layer (flash attn type)
                                causal_mask = create_causal_mask(config=self.config, input_embeds=hidden_states, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=text_position_ids) 
                                # NOTE-ZY: ç”¨è¿™ä¸ªè¿˜æ˜¯ä¸‹é¢çš„ï¼ˆvideo-r1ç¯å¢ƒçš„å®ç°ï¼‰ -> éƒ½èƒ½æ­£å¸¸inferï¼Œè¿˜æ˜¯ä¼˜å…ˆç”¨officialçš„å®ç°
                                # if attention_mask is not None and 0.0 in attention_mask:
                                #     causal_mask = attention_mask
                                # else:
                                #     causal_mask = None
                            else: # continue to be eager layer type
                                config_temp = copy.deepcopy(self.config)
                                config_temp._attn_implementation = "eager"
                                text_position_ids=position_ids[0]
                                assert hidden_states.shape[1] == text_position_ids.shape[1], f"hidden_states.shape[1] = {hidden_states.shape[1]} != text_position_ids.shape[1] = {text_position_ids.shape[1]}"
                                causal_mask_temp = create_causal_mask(config=config_temp, input_embeds=hidden_states, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=text_position_ids, compression_prefill=False, compression_decode=True)
                                causal_mask = causal_mask_temp
                                
                        position_embeddings = self.rotary_emb(hidden_states, position_ids) # computed w.r.t. position_ids values, irrelevant to hidden_states values

                        # update cache for next token input
                        self.attention_mask_cache[layer_idx + 1] = attention_mask # [item.shape[1] for item in self.attention_mask_cache]
                        self.cache_position_cache[layer_idx + 1] = cache_position # [item.shape[2] for item in self.position_ids_cache], self.position_ids_cache[0], self.position_ids_cache[-1] 
                        self.position_ids_cache[layer_idx + 1] = position_ids # [item[-1] for item in self.cache_position_cache]
                        self.boundaries_cache[layer_idx + 1][2] += 1 # self.boundaries_cache, boundary at each layer keeps the same, except that the seq length + 1
                    else: 
                        # prefill phrase
                        # print(f"at prefill stage: layer_idx = {layer_idx} with pruning")
                        # set random scores if nan appears so that no protected tokens are pruned
                        if torch.isnan(importance_score).any().item():
                            print("Got nan in importance_score!")
                            importance_score = torch.randn_like(importance_score).fill_(-1.0)

                        # keep the important tokens that are also valid current layer (#tokens are decreasing over layers)                        
                        keep_num = int(((v_cnt_before_llm * self.retain_rate[layer_idx]).long() + protected_num).item()) # (importance_score.shape[1] - protected_num) * self.retain_rate[layer_idx] + protected_num
                            # v_cnt_before_llm = 240, self.retain_rate[layer_idx] = 0.8 -> 240*0.8 = 192 # NOTE-ZY: è¿™é‡Œ v_cnt_before_llm å®šä¹‰æ˜¯è¾“å…¥çš„visual tokenæ€»æ•°
                            # protected_num = 90 # éè§†è§‰token NOTE-ZY: è¿™é‡Œé¢åŒ…æ‹¬äº† <vstart> <vend>, å› ä¸º240+90=330
                            # keep_num = 192+90 = 282
                        assert hidden_states.shape[0] == 1, f"Batch size > 1 (bs={hidden_states.shape[0]}) æš‚ä¸æ”¯æŒ"
                        protected_token_indices = protected_inds[:, 1] 
                            # shape: (num_protected) e.g., torch.Size([90])
                            # tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  41,  42,  43,
                            #     44,  45,  46,  47,  48,  49,  80,  81,  82,  83,  84,  85,  86,  87,
                            #     88,  89, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 160, 161,
                            #     162, 163, 164, 165, 166, 167, 168, 169, 200, 201, 202, 203, 204, 205,
                            #     206, 207, 208, 209, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249,
                            #     280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 320, 321, 322, 323,
                            #     324, 325, 326, 327, 328, 329], device='cuda:0')]
                        
                        # NOTE-ZY: æ¯æ¬¡åº”è¯¥ protected_token_indices.shape æ˜¯ç›¸åŒçš„
                            # print(f"boundaries = {boundaries}")
                            # print(f"protected_token_indices = {protected_token_indices}")
                            # print(f"protected_token_indices.shape = {protected_token_indices.shape}")
                            # print(f"importance_score.shape = {importance_score.shape}")
                            # boundaries = [[(10, 41), (49, 80), (89, 120), (129, 160), (169, 200), (209, 240), (249, 280), (289, 320)], 321, 330]
                            # protected_token_indices = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  41,  42,  43,
                            #         44,  45,  46,  47,  48,  49,  80,  81,  82,  83,  84,  85,  86,  87,
                            #         88,  89, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 160, 161,
                            #         162, 163, 164, 165, 166, 167, 168, 169, 200, 201, 202, 203, 204, 205,
                            #         206, 207, 208, 209, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249,
                            #         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 320, 321, 322, 323,
                            #         324, 325, 326, 327, 328, 329], device='cuda:0')
                            # protected_token_indices.shape = torch.Size([90])
                            # importance_score.shape = torch.Size([1, 330])
                            
                        importance_score[0, protected_token_indices] = 1.0 
                            # importance_score: (B, N), e.g., torch.Size([1, 330])
                            # importance_score[0, protected_token_indices].shape = torch.Size([90])
                            # set scores to 1.0 for text tokens that should be protected -> å› ä¸ºé™åºï¼Œtextual tokenå¿…å®šèƒ½è¢«ä¿ç•™
                        _, inds = torch.sort(importance_score, dim=-1, descending=True) # é™åºï¼Œé€‰æ‹©è¦è¢«pruning token
                        
                        # the indices we kept
                        keep_inds_1d = inds[0, :keep_num].to(hidden_states.device) # torch.Size([1, 282])
                        
                        # è®¡ç®—è¢«åˆ é™¤çš„ token ç´¢å¼•
                        original_num_tokens = importance_score.shape[1]
                        all_indices_set = set(range(original_num_tokens))
                        keep_indices_set = set(keep_inds_1d.cpu().numpy())
                        deleted_indices_set = all_indices_set - keep_indices_set
                        
                        keep_inds_1d_sorted, _ = torch.sort(keep_inds_1d, dim=-1) # æ¢å¤é¡ºåº

                        # keep the chosen token embeddings, attention masks, position ids
                        hidden_states = hidden_states[0, keep_inds_1d_sorted].unsqueeze(0) # (1, N, C), e.g., torch.Size([1, 282, 4096])
                        if attention_mask is not None: attention_mask = attention_mask[0, keep_inds_1d_sorted].unsqueeze(0) # (1, N), e.g., torch.Size([1, 282])
                        position_ids = position_ids[:, 0, keep_inds_1d_sorted].unsqueeze(1) # (3, 1, N), e.g., torch.Size([3, 1, 282])
                        max_len = hidden_states.size(1) # find max length among all samples in the batch
                                                    
                        # adjust boundaries for next layer 
                        # import pdb; pdb.set_trace()
                        adjusted_boundaries = boundaries # [[å¤šå¯¹ [<|vision_start|>index, <|vision_end|>index]], q_start_idx, #tokens]
                            # adjusted_boundaries[0] -> v star, v end pairs 
                            # NOTE-ZY: pruningä¹‹åï¼Œæ¯ç»„çš„v start idxã€v end idxç†åº”è¿›è¡Œè°ƒæ•´
                        
                        # è°ƒæ•´ Boundaries
                        adjusted_boundaries = boundaries 
                        new_vision_boundaries = []
                        cumulative_reduct_cnt = 0 # è®°å½•åˆ°ç›®å‰ä¸ºæ­¢æ€»å…±åˆ é™¤äº†å¤šå°‘ token
                        for v_start, v_end in adjusted_boundaries[0]:
                            # v_start çš„æ–°ä½ç½®ï¼Œå–å†³äº *åœ¨å®ƒä¹‹å‰* åˆ é™¤äº†å¤šå°‘ tokenã€‚
                            # æ­¤æ—¶çš„ cumulative_reduct_cnt æ­£æ˜¯è¿™ä¸ªå€¼ (å³ä¸Šä¸€è½®å¾ªç¯ç»“æŸæ—¶çš„æ€»æ•°)ã€‚
                            new_v_start = v_start - cumulative_reduct_cnt
                            # è®¡ç®— *å½“å‰* å¸§å†…éƒ¨ (v_start + 1, v_end) æœ‰å¤šå°‘ token è¢«åˆ é™¤äº†
                            current_frame_reduct_cnt = 0
                            for idx in range(v_start + 1, v_end):
                                if idx in deleted_indices_set:
                                    current_frame_reduct_cnt += 1

                            # æ›´æ–° *æ€»* åˆ é™¤è®¡æ•°ï¼ŒåŠ ä¸Šå½“å‰å¸§çš„åˆ é™¤æ•°
                            cumulative_reduct_cnt += current_frame_reduct_cnt

                            # v_end çš„æ–°ä½ç½®ï¼Œå–å†³äº *åˆ°ç›®å‰ä¸ºæ­¢* (åŒ…æ‹¬å½“å‰å¸§) æ€»å…±åˆ é™¤äº†å¤šå°‘ token
                            new_v_end = v_end - cumulative_reduct_cnt
                            new_vision_boundaries.append([new_v_start, new_v_end])

                        # æ›´æ–° vision boundaries åˆ—è¡¨
                        adjusted_boundaries[0] = new_vision_boundaries

                        # æ›´æ–° q_start å’Œ total_tokens ---
                        # å¾ªç¯ç»“æŸæ—¶ï¼Œcumulative_reduct_cnt == total_reduct_cnt
                        # è¿™é‡Œçš„ total_reduct_cnt åº”è¯¥ç­‰äº len(deleted_indices_set)
                        total_reduct_cnt = cumulative_reduct_cnt 

                        _expected_reduct_cnt = importance_score.shape[1] - hidden_states.shape[1]
                        assert total_reduct_cnt == _expected_reduct_cnt, "Pruning è®¡æ•°ä¸ä¸€è‡´" 
                                                   
                        reduct_cnt = importance_score.shape[1] - hidden_states.shape[1] 
                            # 330 - 282 = 48, éƒ½æ˜¯ä»visual tokenä¸­pruningçš„
                        adjusted_boundaries[1] = adjusted_boundaries[1] - reduct_cnt 
                            # question tokens move right (padding) and then move left (visual compression)
                            # 321 - 48 = 273 -> q start idx
                        adjusted_boundaries[2] = adjusted_boundaries[2] - reduct_cnt 
                            # end+1 token move right (padding) and then move left (visual compression)
                            # 330 - 48 = 282 -> num_token
                        assert hidden_states.size(1) == adjusted_boundaries[2], "Pruning åçš„ token æ•°é‡ä¸åŒ¹é…"
                        
                        # outputs to next layer
                        # hidden_states = hidden_states
                        self.attention_mask_cache[layer_idx + 1] = attention_mask
                        self.position_ids_cache[layer_idx + 1] = position_ids
                        self.boundaries_cache[layer_idx + 1] = adjusted_boundaries
                        cache_position = cache_position[:max_len] # during prefill, cache_position variable keeps the same across layers (default), yet keeps changed when doing token compression
                        self.cache_position_cache[layer_idx + 1] = cache_position # we have #max_len tokens to process in next layer (cache is independently managed at each layer)

                        # re-compute critical variables for next layer
                        if (layer_idx + 1) < len(self.layers):
                            if self.layers[layer_idx + 1].self_attn_type != 'eager': # layer transition: recompute mask for next layer (flash attn type)
                                causal_mask = create_causal_mask(config=self.config, input_embeds=hidden_states, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=text_position_ids) 
                                # NOTE-ZY: ç”¨è¿™ä¸ªè¿˜æ˜¯ä¸‹é¢çš„ï¼ˆvideo-r1ç¯å¢ƒçš„å®ç°ï¼‰ -> éƒ½èƒ½æ­£å¸¸inferï¼Œè¿˜æ˜¯ä¼˜å…ˆç”¨officialçš„å®ç°
                                # if attention_mask is not None and 0.0 in attention_mask:
                                #     causal_mask = attention_mask
                                # else:
                                #     causal_mask = None
                            else: # continue to be eager layer type
                                config_temp = copy.deepcopy(self.config)
                                config_temp._attn_implementation = "eager"
                                text_position_ids=position_ids[0]
                                assert hidden_states.shape[1] == text_position_ids.shape[1], f"hidden_states.shape[1] = {hidden_states.shape[1]} != text_position_ids.shape[1] = {text_position_ids.shape[1]}"
                                causal_mask_temp = create_causal_mask(config=config_temp, input_embeds=hidden_states, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=text_position_ids, compression_prefill=True, compression_decode=False)
                                causal_mask = causal_mask_temp # NOTE-ZY: torch.Size([1, 1, 282, 282])ï¼Œé€‚é…åˆšåˆšçš„token pruning
                        
                        position_embeddings = self.rotary_emb(hidden_states, position_ids) # computed w.r.t. position_ids values, irrelevant to hidden_states values
                        text_position_ids = position_ids[0]
                            # hidden_states.shape torch.Size([3, 1, 282])
                            # position_ids.shape torch.Size([3, 1, 282])
                            # update position_embeddings -> position_embeddings[0].shape = torch.Size([1, 330, 128])

                        protected_inds, protected_num = get_protected_info(boundaries, hidden_states) 
                            # print(f"boundaries = {boundaries}")
                            # print(f"hidden_states.shape = {hidden_states.shape}")
                            # print(f"protected_inds.shape = {protected_inds.shape}")
                            # print(f"protected_num = {protected_num}")
                            # boundaries = [[[10, 26], [34, 61], [70, 100], [109, 131], [140, 165], [174, 200], [209, 237], [246, 272]], 273, 282]
                            # hidden_states.shape = torch.Size([1, 282, 4096])
                            # protected_inds.shape = torch.Size([90, 2])
                            # protected_num = tensor([90], device='cuda:0')

                layer_idx += 1
            ############# token pruning #############
            
        hidden_states = self.norm(hidden_states)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
        )

    def _deepstack_process(
        self, hidden_states: torch.Tensor, visual_pos_masks: torch.Tensor, visual_embeds: torch.Tensor
    ):
        visual_pos_masks = visual_pos_masks.to(hidden_states.device)
        visual_embeds = visual_embeds.to(hidden_states.device, hidden_states.dtype)
        local_this = hidden_states[visual_pos_masks, :].clone() + visual_embeds
        hidden_states[visual_pos_masks, :] = local_this
        return hidden_states

@auto_docstring
class Qwen3VLModel(Qwen3VLPreTrainedModel):
    base_model_prefix = ""
    _checkpoint_conversion_mapping = {}
    # Reference: fix gemma3 grad acc #37208
    accepts_loss_kwargs = False
    config: Qwen3VLConfig
    _no_split_modules = ["Qwen3VLTextDecoderLayer", "Qwen3VLVisionBlock"]

    def __init__(self, config):
        super().__init__(config)
        self.visual = Qwen3VLVisionModel._from_config(config.vision_config)
        self.language_model = Qwen3VLTextModel._from_config(config.text_config)
        self.rope_deltas = None  # cache rope_deltas here

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.language_model = decoder

    def get_decoder(self):
        return self.language_model

    def get_rope_index(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Different from the original implementation, Qwen3VL use timestamps rather than absolute time position ids."""

        # Since we use timestamps to seperate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split
        if video_grid_thw is not None:
            video_grid_thw = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)
            video_grid_thw[:, 0] = 1

        spatial_merge_size = self.config.vision_config.spatial_merge_size
        image_token_id = self.config.image_token_id
        video_token_id = self.config.video_token_id
        vision_start_token_id = self.config.vision_start_token_id
        mrope_position_deltas = []
        if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):
            total_input_ids = input_ids
            if attention_mask is None:
                attention_mask = torch.ones_like(total_input_ids)
            position_ids = torch.ones(
                3,
                input_ids.shape[0],
                input_ids.shape[1],
                dtype=input_ids.dtype,
                device=input_ids.device,
            )
            image_index, video_index = 0, 0
            attention_mask = attention_mask.to(total_input_ids.device)
            for i, input_ids in enumerate(total_input_ids):
                input_ids = input_ids[attention_mask[i] == 1]
                image_nums, video_nums = 0, 0
                vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)
                vision_tokens = input_ids[vision_start_indices + 1]
                image_nums = (vision_tokens == image_token_id).sum()
                video_nums = (vision_tokens == video_token_id).sum()
                input_tokens = input_ids.tolist()
                llm_pos_ids_list: list = []
                st = 0
                remain_images, remain_videos = image_nums, video_nums
                for _ in range(image_nums + video_nums):
                    if image_token_id in input_tokens and remain_images > 0:
                        ed_image = input_tokens.index(image_token_id, st)
                    else:
                        ed_image = len(input_tokens) + 1
                    if video_token_id in input_tokens and remain_videos > 0:
                        ed_video = input_tokens.index(video_token_id, st)
                    else:
                        ed_video = len(input_tokens) + 1
                    if ed_image < ed_video:
                        t, h, w = (
                            image_grid_thw[image_index][0],
                            image_grid_thw[image_index][1],
                            image_grid_thw[image_index][2],
                        )
                        image_index += 1
                        remain_images -= 1
                        ed = ed_image

                    else:
                        t, h, w = (
                            video_grid_thw[video_index][0],
                            video_grid_thw[video_index][1],
                            video_grid_thw[video_index][2],
                        )
                        video_index += 1
                        remain_videos -= 1
                        ed = ed_video
                    llm_grid_t, llm_grid_h, llm_grid_w = (
                        t.item(),
                        h.item() // spatial_merge_size,
                        w.item() // spatial_merge_size,
                    )
                    text_len = ed - st

                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)

                    # t_index is always 0 because llm_grid_t is always 1 (we use timestamps to encode the temporal information for videos)
                    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(-1, llm_grid_h * llm_grid_w).flatten()
                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()
                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()
                    llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)
                    st = ed + llm_grid_t * llm_grid_h * llm_grid_w

                if st < len(input_tokens):
                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
                    text_len = len(input_tokens) - st
                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)

                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)
                mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))
            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)
            return position_ids, mrope_position_deltas
        else:
            if attention_mask is not None:
                position_ids = attention_mask.long().cumsum(-1) - 1
                position_ids.masked_fill_(attention_mask == 0, 1)
                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(attention_mask.device)
                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]
                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
            else:
                position_ids = (
                    torch.arange(input_ids.shape[1], device=input_ids.device)
                    .view(1, 1, -1)
                    .expand(3, input_ids.shape[0], -1)
                )
                mrope_position_deltas = torch.zeros(
                    [input_ids.shape[0], 1],
                    device=input_ids.device,
                    dtype=input_ids.dtype,
                )

            return position_ids, mrope_position_deltas

    def get_video_features(
        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None
    ):
        """
        Encodes videos into continuous embeddings that can be forwarded to the language model. The deepstack visual features are also returned.

        Args:
            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
                The tensors corresponding to the input videos.
            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
                The temporal, height and width of feature shape of each video in LLM.
        """
        # Same implementation as for images
        return self.get_image_features(pixel_values_videos, video_grid_thw)

    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):
        """
        Encodes images into continuous embeddings that can be forwarded to the language model. The deepstack visual features are also returned.

        Args:
            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
                The tensors corresponding to the input images.
            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
                The temporal, height and width of feature shape of each image in LLM.
        """
        pixel_values = pixel_values.type(self.visual.dtype)
        image_embeds, deepstack_image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)
        split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
        image_embeds = torch.split(image_embeds, split_sizes)
        return image_embeds, deepstack_image_embeds

    def get_placeholder_mask(
        self,
        input_ids: torch.LongTensor,
        inputs_embeds: torch.FloatTensor,
        image_features: Optional[torch.FloatTensor] = None,
        video_features: Optional[torch.FloatTensor] = None,
    ):
        """
        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is
        equal to the length of multimodal features. If the lengths are different, an error is raised.
        """
        if input_ids is None:
            special_image_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_image_mask = special_image_mask.all(-1)
            special_video_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_video_mask = special_video_mask.all(-1)
        else:
            special_image_mask = input_ids == self.config.image_token_id
            special_video_mask = input_ids == self.config.video_token_id

        n_image_tokens = special_image_mask.sum()
        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():
            raise ValueError(
                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}"
            )

        n_video_tokens = special_video_mask.sum()
        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():
            raise ValueError(
                f"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}"
            )

        return special_image_mask, special_video_mask

    @auto_docstring
    @check_model_inputs
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Qwen3VLModelOutputWithPast]:
        r"""
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.
        """
        
        # print(f"Qwen3VLModel.forward()å¼€å¤´ -> input_ids.shape = {input_ids.shape}")
        
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        # import pdb; pdb.set_trace()
        is_prefill = past_key_values is None or past_key_values.get_seq_length() == 0
        rl_forward = is_prefill and attention_mask is None
        run_our_forward = True # self.language_model.run_our_forward
        
        boundaries = None
        vis_grid_thw = None

        # import pdb; pdb.set_trace()
        # æ ¸å¿ƒçš„token merging # LLM
        if inputs_embeds is None:
            inputs_embeds = self.get_input_embeddings()(input_ids) # å·²ç»è¿›è¡Œè¿‡token compressionçš„input_ids -> å·²ç»è¿›è¡Œè¿‡token compressionçš„inputs_embeds -> torch.Size([1, 330, 4096])

        image_mask = None
        video_mask = None
        
        # prefilling stage or RL stage get_per_token_logps() # å›¾ç‰‡å’Œvideoæ“ä½œåŸºæœ¬ä¸€è‡´ï¼Œä¼¼ä¹ç›´æ¥å¤åˆ¶å°±è¡Œ
        if pixel_values is not None:
            if not run_our_forward: # original code
                image_embeds, deepstack_image_embeds = self.get_image_features(pixel_values, image_grid_thw)
                image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
                image_mask, _ = self.get_placeholder_mask(
                    input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds
                )
                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
            else:
                # NOTE-ZY: imageä»£ç ç®€ç­”æ”¹è‡ªvideo
                                # ç”¨qwen3vlçš„æ“ä½œæ›¿æ¢qwen2.5vl-r1çš„æ“ä½œ
                image_embeds, deepstack_image_embeds = self.get_image_features(pixel_values, image_grid_thw)
                image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
                                
                ############# Token Merging with Pre-computed Input IDs #############
                # Token merging: keep unique tokens and their position info, without re-assignment
                # NOTE-ZY: Simply use bs=1 for each gpu during training & inference
                assert input_ids.shape[0] == 1, f"Batch size > 1 (bs={input_ids.shape[0]}) æš‚ä¸æ”¯æŒ"
                
                # import pdb; pdb.set_trace()
                vis_token_id = self.config.image_token_id
                vision_start_token_id = self.config.vision_start_token_id
                vision_end_token_id = self.config.vision_end_token_id
                vis_grid_thw_b = image_grid_thw[0] # tensor([ 8, 16, 30], device='cuda:0')
                
                # perform spatial token merging within each frame
                # checked: image_processor.merge_size = video_processor.merge_size = 2
                T = vis_grid_thw_b[0].item() # e.g., 8
                H = vis_grid_thw_b[1].item() // self.config.vision_config.spatial_merge_size # e.g., 8
                W = vis_grid_thw_b[2].item() // self.config.vision_config.spatial_merge_size # e.g., 15
                C = inputs_embeds.size(-1)
                vis_feats = einops.rearrange(image_embeds, "(T H W) C -> T (H W) C", T=T, H=H) # encoder_vis_embeds_b[0] # torch.Size([8, 120, 4096])
                token_idx = torch.arange(T * H * W, device=vis_feats.device).reshape(T, -1, 1) # (T, H, W) the index covers the whole image # torch.Size([8, 120, 1])
                vis_feats, token_idx = bipartite_soft_matching_merge(metric=vis_feats, r=vis_feats.shape[1]//2, x=vis_feats, token_idx=token_idx) # 50% 
                    # vis_feats.shape = torch.Size([8, 60, 4096]), token_idx.shape = torch.Size([8, 60, 1])
                vis_feats, token_idx = bipartite_soft_matching_merge(metric=vis_feats, r=vis_feats.shape[1]//2, x=vis_feats, token_idx=token_idx) # 25% 
                    # vis_feats.shape = torch.Size([8, 30, 4096]), token_idx.shape = torch.Size([8, 30, 1])
                
                # the visual embeddings and indices after merging
                token_idx = token_idx.reshape(-1) # the indices within visual token scope # è®°å½•è¢«ä¿ç•™çš„token -> ä¸ºäº†åé¢ç®—pos -> æ¢å¤ä¸ºåŸæœ¬çš„pos # torch.Size([240])

                input_ids_1d = input_ids[0]  # shape = (L,)
                if attention_mask is not None:
                    attention_mask_1d = attention_mask[0]  # shape = (L,)
                else:
                    attention_mask_1d = None # Handle case where attention_mask is None
                
                # NOTE-15: during RL stage get_per_token_logps(), the generated tokens might be visual tokens, which disturbs boundary computation here
                # NOTE-ZY: original boundary definition
                # æ‰¾åˆ°æ‰€æœ‰çš„ [<|vision_start|>index, <|vision_end|>index] çš„pairsï¼Œæ ‡æ³¨æ¯ä¸€å¸§çš„èµ·ç‚¹ä¸ç»ˆç‚¹
                vision_start_token_idx = (torch.where(input_ids_1d == vision_start_token_id)[0]).tolist()  # list of idx
                vision_end_token_idx = (torch.where(input_ids_1d == vision_end_token_id)[0]).tolist()  # list of idx
                vision_start_end_idx_pairs = list(zip(vision_start_token_idx, vision_end_token_idx))  # list of (start_idx, end_idx)
                assert len(vision_start_end_idx_pairs) == T, f"æ‰¾åˆ°çš„ <|vision_start|>/<|vision_end|> pairs æ•°é‡ ({len(vision_start_end_idx_pairs)}) ä¸ T ({T}) ä¸åŒ¹é…"
                # æ‰¾åˆ°æœ€åä¸€ä¸ª <|vision_end|>ï¼Œå³ä¸º question token start idx ã€æœ«å°¾çš„éè§†è§‰tokenã€‘
                q_start_idx = max(vision_end_token_idx) + 1
                # ä¿®æ”¹boundaryä¸º [[å¤šå¯¹ [<|vision_start|>index, <|vision_end|>index]], q_start_idx, #tokens]
                num_tokens = input_ids.shape[1]
                boundaries_b = [vision_start_end_idx_pairs, q_start_idx, num_tokens] # boundaryï¼Œæ­¤æ—¶input_idså·²ç»æœ‰å‹ç¼©vis token
                
                updated_embeds_chunks = []
                updated_raw_ids_chunks = []
                updated_raw_mask_chunks = []
                
                # vis_feats shape is [T, Merged_H_W, C] (e.g., [8, 30, 4096])
                merged_embeds_per_frame = vis_feats 
                
                # Raw (unmerged) visual tokens
                raw_vis_ids_per_frame = torch.full((H * W,), vis_token_id, dtype=input_ids.dtype, device=input_ids.device) # shape [120]
                if attention_mask_1d is not None:
                    raw_vis_mask_per_frame = torch.full((H * W,), 1, dtype=attention_mask.dtype, device=attention_mask.device) # shape [120]

                last_idx = 0
                for i in range(T):
                    start_idx = vision_start_end_idx_pairs[i][0]
                    end_idx = vision_start_end_idx_pairs[i][1]
                    
                    # inputs_embeds_b
                    updated_embeds_chunks.append(inputs_embeds[0, last_idx:start_idx])      # æ·»åŠ æ­¤ image chunk ä¹‹å‰çš„ æ–‡æœ¬éƒ¨åˆ† (e.g., <|im_start|>user\n<19.6 seconds>)
                    updated_embeds_chunks.append(inputs_embeds[0, start_idx:start_idx+1])   # <|vision_start|>
                    updated_embeds_chunks.append(merged_embeds_per_frame[i])                # token compressionåçš„30ä¸ª<|image_pad|>
                    updated_embeds_chunks.append(inputs_embeds[0, end_idx:end_idx+1])       # <|vision_end|>
                    
                    # raw_input_ids_b
                    updated_raw_ids_chunks.append(input_ids_1d[last_idx:start_idx])
                    updated_raw_ids_chunks.append(input_ids_1d[start_idx:start_idx+1])
                    updated_raw_ids_chunks.append(raw_vis_ids_per_frame)                    # æœªè¿›è¡Œtoken compressionçš„120ä¸ª<|image_pad|>çš„input_ids
                    updated_raw_ids_chunks.append(input_ids_1d[end_idx:end_idx+1])

                    # raw_attention_mask_b
                    if attention_mask_1d is not None:
                        updated_raw_mask_chunks.append(attention_mask_1d[last_idx:start_idx])
                        updated_raw_mask_chunks.append(attention_mask_1d[start_idx:start_idx+1])
                        updated_raw_mask_chunks.append(raw_vis_mask_per_frame)              # æœªè¿›è¡Œtoken compressionçš„120ä¸ª<|image_pad|>çš„masks
                        updated_raw_mask_chunks.append(attention_mask_1d[end_idx:end_idx+1])
                        
                    # æ›´æ–°ä¸‹ä¸€ä¸ªæ–‡æœ¬å—çš„èµ·å§‹ç´¢å¼•
                    last_idx = end_idx + 1

                # æ·»åŠ æœ€åä¸€ä¸ª image chunk ä¹‹åçš„ æ–‡æœ¬éƒ¨åˆ† (e.g., Describe this image.<|im_end|>\n<|im_start|>assistant\n)
                updated_embeds_chunks.append(inputs_embeds[0, last_idx:])
                inputs_embeds_b = torch.cat(updated_embeds_chunks, dim=0).unsqueeze(0) # [1, L, C], e.g., torch.Size([1, 330, 4096])
                
                updated_raw_ids_chunks.append(input_ids_1d[last_idx:])
                raw_input_ids_b = torch.cat(updated_raw_ids_chunks, dim=0).unsqueeze(0) # [1, Raw_L], e.g., torch.Size([1, 1050, 4096])

                # convert attention_mask to the length before any token reduction
                if attention_mask_1d is not None: # prefill stage
                    updated_raw_mask_chunks.append(attention_mask_1d[last_idx:])
                    raw_attention_mask_b = torch.cat(updated_raw_mask_chunks, dim=0).unsqueeze(0) # [1, Raw_L]
                else: # RL stage get_per_token_logps() # prefillå†…æœ‰ä¸ªå­åˆ†æ”¯å…³äºRL
                    raw_attention_mask_b = None
                    
                # import pdb; pdb.set_trace()
                inputs_embeds = inputs_embeds_b 
                    # transformers/models/qwen3_vl/processing_qwen3_vl.py has already align their length  
                    # e.g., torch.Size([1, 330, 4096])
                raw_input_ids = raw_input_ids_b
                    # e.g., torch.Size([1, 1050])
                raw_attention_mask = raw_attention_mask_b
                    # e.g., torch.Size([1, 1050, 4096])
                boundaries = boundaries_b 
                    # [[å¤šå¯¹ [<|vision_start|>index, <|vision_end|>index]], #tokens]
                    # e.g., [[(10, 41), (49, 80), (89, 120), (129, 160), (169, 200), (209, 240), (249, 280), (289, 320)], 321, 330]
                vis_grid_thw = image_grid_thw 
                
        # prefilling stage or RL stage get_per_token_logps()
        if pixel_values_videos is not None: # è§†é¢‘
            if not run_our_forward: # original code
                video_embeds, deepstack_video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)
                video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
                _, video_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds)
                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)
            else:
                # ç”¨qwen3vlçš„æ“ä½œæ›¿æ¢qwen2.5vl-r1çš„æ“ä½œ
                video_embeds, deepstack_video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw) 
                    # len(deepstack_video_embeds)=3, deepstack_video_embeds[0].shape = torch.Size([960, 4096])
                video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
                    # torch.Size([960, 4096]) æœªè¿›è¡Œtoken mergingçš„video_embeds
                                
                ############# Token Merging with Pre-computed Input IDs #############
                # Token merging: keep unique tokens and their position info, without re-assignment
                # NOTE-ZY: Simply use bs=1 for each gpu during training & inference
                assert input_ids.shape[0] == 1, f"Batch size > 1 (bs={input_ids.shape[0]}) æš‚ä¸æ”¯æŒ"
                
                # import pdb; pdb.set_trace()
                vis_token_id = self.config.video_token_id
                vision_start_token_id = self.config.vision_start_token_id
                vision_end_token_id = self.config.vision_end_token_id
                vis_grid_thw_b = video_grid_thw[0] # tensor([ 8, 16, 30], device='cuda:0')
                
                # perform spatial token merging within each frame
                # checked: image_processor.merge_size = video_processor.merge_size = 2
                T = vis_grid_thw_b[0].item() # e.g., 8
                H = vis_grid_thw_b[1].item() // self.config.vision_config.spatial_merge_size # e.g., 8
                W = vis_grid_thw_b[2].item() // self.config.vision_config.spatial_merge_size # e.g., 15
                C = inputs_embeds.size(-1)
                vis_feats = einops.rearrange(video_embeds, "(T H W) C -> T (H W) C", T=T, H=H) # encoder_vis_embeds_b[0] # torch.Size([8, 120, 4096])
                token_idx = torch.arange(T * H * W, device=vis_feats.device).reshape(T, -1, 1) # (T, H, W) the index covers the whole video # torch.Size([8, 120, 1])
                vis_feats, token_idx = bipartite_soft_matching_merge(metric=vis_feats, r=vis_feats.shape[1]//2, x=vis_feats, token_idx=token_idx) # 50% 
                    # vis_feats.shape = torch.Size([8, 60, 4096]), token_idx.shape = torch.Size([8, 60, 1])
                vis_feats, token_idx = bipartite_soft_matching_merge(metric=vis_feats, r=vis_feats.shape[1]//2, x=vis_feats, token_idx=token_idx) # 25% 
                    # vis_feats.shape = torch.Size([8, 30, 4096]), token_idx.shape = torch.Size([8, 30, 1])
                
                # the visual embeddings and indices after merging
                token_idx = token_idx.reshape(-1) # the indices within visual token scope # è®°å½•è¢«ä¿ç•™çš„token -> ä¸ºäº†åé¢ç®—pos -> æ¢å¤ä¸ºåŸæœ¬çš„pos # torch.Size([240])

                input_ids_1d = input_ids[0]  # shape = (L,)
                if attention_mask is not None:
                    attention_mask_1d = attention_mask[0]  # shape = (L,)
                else:
                    attention_mask_1d = None # Handle case where attention_mask is None
                
                # NOTE-15: during RL stage get_per_token_logps(), the generated tokens might be visual tokens, which disturbs boundary computation here
                # NOTE-ZY: original boundary definition
                # æ‰¾åˆ°æ‰€æœ‰çš„ [<|vision_start|>index, <|vision_end|>index] çš„pairsï¼Œæ ‡æ³¨æ¯ä¸€å¸§çš„èµ·ç‚¹ä¸ç»ˆç‚¹
                vision_start_token_idx = (torch.where(input_ids_1d == vision_start_token_id)[0]).tolist()  # list of idx
                vision_end_token_idx = (torch.where(input_ids_1d == vision_end_token_id)[0]).tolist()  # list of idx
                vision_start_end_idx_pairs = list(zip(vision_start_token_idx, vision_end_token_idx))  # list of (start_idx, end_idx)
                assert len(vision_start_end_idx_pairs) == T, f"æ‰¾åˆ°çš„ <|vision_start|>/<|vision_end|> pairs æ•°é‡ ({len(vision_start_end_idx_pairs)}) ä¸ T ({T}) ä¸åŒ¹é…"
                # æ‰¾åˆ°æœ€åä¸€ä¸ª <|vision_end|>ï¼Œå³ä¸º question token start idx ã€æœ«å°¾çš„éè§†è§‰tokenã€‘
                q_start_idx = max(vision_end_token_idx) + 1
                # ä¿®æ”¹boundaryä¸º [[å¤šå¯¹ [<|vision_start|>index, <|vision_end|>index]], q_start_idx, #tokens]
                num_tokens = input_ids.shape[1]
                boundaries_b = [vision_start_end_idx_pairs, q_start_idx, num_tokens] # boundaryï¼Œæ­¤æ—¶input_idså·²ç»æœ‰å‹ç¼©vis token
                
                updated_embeds_chunks = []
                updated_raw_ids_chunks = []
                updated_raw_mask_chunks = []
                
                # vis_feats shape is [T, Merged_H_W, C] (e.g., [8, 30, 4096])
                merged_embeds_per_frame = vis_feats 
                
                # Raw (unmerged) visual tokens
                raw_vis_ids_per_frame = torch.full((H * W,), vis_token_id, dtype=input_ids.dtype, device=input_ids.device) # shape [120]
                if attention_mask_1d is not None:
                    raw_vis_mask_per_frame = torch.full((H * W,), 1, dtype=attention_mask.dtype, device=attention_mask.device) # shape [120]

                last_idx = 0
                for i in range(T):
                    start_idx = vision_start_end_idx_pairs[i][0]
                    end_idx = vision_start_end_idx_pairs[i][1]
                    
                    # inputs_embeds_b
                    updated_embeds_chunks.append(inputs_embeds[0, last_idx:start_idx])      # æ·»åŠ æ­¤ video chunk ä¹‹å‰çš„ æ–‡æœ¬éƒ¨åˆ† (e.g., <|im_start|>user\n<19.6 seconds>)
                    updated_embeds_chunks.append(inputs_embeds[0, start_idx:start_idx+1])   # <|vision_start|>
                    updated_embeds_chunks.append(merged_embeds_per_frame[i])                # token compressionåçš„30ä¸ª<|video_pad|>
                    updated_embeds_chunks.append(inputs_embeds[0, end_idx:end_idx+1])       # <|vision_end|>
                    
                    # raw_input_ids_b
                    updated_raw_ids_chunks.append(input_ids_1d[last_idx:start_idx])
                    updated_raw_ids_chunks.append(input_ids_1d[start_idx:start_idx+1])
                    updated_raw_ids_chunks.append(raw_vis_ids_per_frame)                    # æœªè¿›è¡Œtoken compressionçš„120ä¸ª<|video_pad|>çš„input_ids
                    updated_raw_ids_chunks.append(input_ids_1d[end_idx:end_idx+1])

                    # raw_attention_mask_b
                    if attention_mask_1d is not None:
                        updated_raw_mask_chunks.append(attention_mask_1d[last_idx:start_idx])
                        updated_raw_mask_chunks.append(attention_mask_1d[start_idx:start_idx+1])
                        updated_raw_mask_chunks.append(raw_vis_mask_per_frame)              # æœªè¿›è¡Œtoken compressionçš„120ä¸ª<|video_pad|>çš„masks
                        updated_raw_mask_chunks.append(attention_mask_1d[end_idx:end_idx+1])
                        
                    # æ›´æ–°ä¸‹ä¸€ä¸ªæ–‡æœ¬å—çš„èµ·å§‹ç´¢å¼•
                    last_idx = end_idx + 1

                # æ·»åŠ æœ€åä¸€ä¸ª video chunk ä¹‹åçš„ æ–‡æœ¬éƒ¨åˆ† (e.g., Describe this video.<|im_end|>\n<|im_start|>assistant\n)
                updated_embeds_chunks.append(inputs_embeds[0, last_idx:])
                inputs_embeds_b = torch.cat(updated_embeds_chunks, dim=0).unsqueeze(0) # [1, L, C], e.g., torch.Size([1, 330, 4096])
                
                updated_raw_ids_chunks.append(input_ids_1d[last_idx:])
                raw_input_ids_b = torch.cat(updated_raw_ids_chunks, dim=0).unsqueeze(0) # [1, Raw_L], e.g., torch.Size([1, 1050, 4096])

                # convert attention_mask to the length before any token reduction
                if attention_mask_1d is not None: # prefill stage
                    updated_raw_mask_chunks.append(attention_mask_1d[last_idx:])
                    raw_attention_mask_b = torch.cat(updated_raw_mask_chunks, dim=0).unsqueeze(0) # [1, Raw_L]
                else: # RL stage get_per_token_logps() # prefillå†…æœ‰ä¸ªå­åˆ†æ”¯å…³äºRL
                    raw_attention_mask_b = None
                    
                # import pdb; pdb.set_trace()
                inputs_embeds = inputs_embeds_b 
                    # transformers/models/qwen3_vl/processing_qwen3_vl.py has already align their length  
                    # e.g., torch.Size([1, 330, 4096])
                raw_input_ids = raw_input_ids_b
                    # e.g., torch.Size([1, 1050])
                raw_attention_mask = raw_attention_mask_b
                    # e.g., torch.Size([1, 1050, 4096])
                boundaries = boundaries_b 
                    # [[å¤šå¯¹ [<|vision_start|>index, <|vision_end|>index]], #tokens]
                    # e.g., [[(10, 41), (49, 80), (89, 120), (129, 160), (169, 200), (209, 240), (249, 280), (289, 320)], 321, 330]
                vis_grid_thw = video_grid_thw 
                ############# Token Merging with Pre-computed Input IDs #############
                
        # NOTE-ZY: deepstackä¼šå—token compressionå½±å“ -> deepstack visual featuresæ²¡æœ‰è¿›è¡Œmergingï¼Œé•¿åº¦ä¸åŒ¹é…
        # NOTE-ZY: å‰é¢token compressionå½±å“å¯¼è‡´ image_mask = None / video_mask = None
        visual_pos_masks = None
        deepstack_visual_embeds = None
        if not run_our_forward: # original code
            if image_mask is not None and video_mask is not None:
                # aggregate visual_pos_masks and deepstack_visual_embeds
                image_mask = image_mask[..., 0]
                video_mask = video_mask[..., 0]
                visual_pos_masks = image_mask | video_mask
                deepstack_visual_embeds = []
                image_mask_joint = image_mask[visual_pos_masks]
                video_mask_joint = video_mask[visual_pos_masks]
                for img_embed, vid_embed in zip(deepstack_image_embeds, deepstack_video_embeds):
                    embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1]).to(img_embed.device)
                    embed_joint[image_mask_joint, :] = img_embed
                    embed_joint[video_mask_joint, :] = vid_embed
                    deepstack_visual_embeds.append(embed_joint)
            elif image_mask is not None:
                image_mask = image_mask[..., 0]
                visual_pos_masks = image_mask
                deepstack_visual_embeds = deepstack_image_embeds
            elif video_mask is not None:
                video_mask = video_mask[..., 0]
                visual_pos_masks = video_mask
                deepstack_visual_embeds = deepstack_video_embeds
        else:
            if pixel_values is not None:
                # NOTE-ZY: å›¾ç‰‡ä»£ç ç®€å•æ”¹è‡ªè§†é¢‘ä»£ç 
                real_visual_pos_masks_for_first_three_layers = (input_ids == self.config.image_token_id) # ä¿®æ”¹image
                visual_pos_masks = real_visual_pos_masks_for_first_three_layers
                deepstack_visual_embeds = [layer_embeds[token_idx] for layer_embeds in deepstack_image_embeds] # ä¿®æ”¹image
            
            if pixel_values_videos is not None:
                # NOTE-ZY: deepstack_visual_embedsç­›é€‰ï¼Œæš‚æ—¶ç”¨selectionçš„æ–¹å¼ï¼Œæ²¡ç”¨pooling
                # input_ids.shape = torch.Size([1, 330])
                # raw_visual_pos_masks = (raw_input_ids == self.config.video_token_id) # shape = torch.Size([1, 1050]) -> 960 true elements
                real_visual_pos_masks_for_first_three_layers = (input_ids == self.config.video_token_id) # real input (after merging), shape = torch.Size([1, 330]) -> 240 true elements
                visual_pos_masks = real_visual_pos_masks_for_first_three_layers 
                deepstack_visual_embeds = [layer_embeds[token_idx] for layer_embeds in deepstack_video_embeds] 
                    # deepstack_video_embeds[0].shape = torch.Size([960, 4096])ï¼Œæ¯å±‚deepstack_video_embedséƒ½è¿›è¡Œç›¸åŒçš„indexé€‰æ‹©
                    # token_idx.shape = torch.Size([240]), e.g., token_idx = tensor([3,   7,  11,  15,  19,  23, ..., 955, 959], device='cuda:0')
                    # deepstack_video_embeds[0][token_idx].shape = torch.Size([240, 4096])
                # import pdb; pdb.set_trace()

        if position_ids is None: # NOTE-ZY: åˆå§‹åŒ–çš„position_ids = None -> åé¢ä½¿ç”¨get_ropeæ¥è·å–3D position_ids
            attention_mask_tensor = (
                attention_mask if not isinstance(attention_mask, dict) else attention_mask["full_attention"]
            )
            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:
                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)
                # Only apply conversion for floating point tensors (inverted masks)
                if attention_mask_tensor.dtype.is_floating_point:
                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min
                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()
            
            # Calculate RoPE index once per generation in the pre-fill stage only.
            # When compiling, we can't check tensor values thus we check only input length
            # It is safe to assume that `length!=1` means we're in pre-fill because compiled
            # models currently cannot do asssisted decoding
            prefill_compiled_stage = is_torchdynamo_compiling() and (
                (input_ids is not None and input_ids.shape[1] != 1)
                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)
            )
            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (
                (cache_position is not None and cache_position[0] == 0)
                or (past_key_values is None or past_key_values.get_seq_length() == 0)
            )
            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:                
                # prefill stage
                # if len(all_raw_input_ids) == 0: # NOTE-ZY: è€ƒè™‘æ¢ä¸€ç§condition
                if not run_our_forward: # original network forward
                    position_ids, rope_deltas = self.get_rope_index(
                        input_ids,
                        image_grid_thw,
                        video_grid_thw,
                        attention_mask=attention_mask_tensor,
                    )
                    self.rope_deltas = rope_deltas
                else:
                    ############# Token Merging with Pre-computed Input IDs ############# 
                    # NOTE-ZY: Simply use bs=1 for each gpu during training & inference
                    assert input_ids.shape[0] == 1, f"Batch size > 1 (bs={input_ids.shape[0]}) æš‚ä¸æ”¯æŒ"
                    # token_idx.shape = torch.Size([240]) # video tokençš„ä½ç½®
                    # raw_input_ids.shape = torch.Size([1, 1050]) # ç”¨äºæå–åŸå§‹çš„rope pos
                    # raw_attention_mask.shape = torch.Size([1, 1050])
                    
                    # visual tokens
                    vis_grid_thw_b = vis_grid_thw[0]
                    T = vis_grid_thw_b[0].item()
                    H = vis_grid_thw_b[1].item() // self.config.vision_config.spatial_merge_size # see Qwen2_5_VLPatchMerger
                    W = vis_grid_thw_b[2].item() // self.config.vision_config.spatial_merge_size # see Qwen2_5_VLPatchMerger
                    raw_token_count_per_frame = H * W  # e.g., 120
                    token_cnt = raw_token_count_per_frame 
                    token_cnt = token_cnt - token_cnt // 2 # e.g., 60
                    token_cnt = token_cnt - token_cnt // 2 # e.g., 30
                    merged_token_count_per_frame = token_cnt
                    reduct_cnt = int(raw_token_count_per_frame - merged_token_count_per_frame) * T # e.g., 90*T
                    
                    # text tokens
                    # boundaries = [vision_start_end_idx_pairs, q_start_idx_merged, num_tokens_merged]
                    vision_start_end_idx_pairs = boundaries[0] # e.g., [(10, 41), ...]
                    q_start_idx_merged = boundaries[1] # e.g., 321
                    # q_tok_start_idx_raw æ˜¯ *raw* (1050) åºåˆ—ä¸­ query token çš„èµ·å§‹ç´¢å¼•
                    q_tok_start_idx_raw = q_start_idx_merged + reduct_cnt # e.g., 321 + 720 = 1041
                    
                    # NOTE: to avoid the generated tokens involving image/video tokens, replace them with others temporily in get_rope_index() function # ä¸ºäº†é˜²æ­¢ä¸€äº›corner case
                    temp_input_ids = raw_input_ids # TODO: å±•ç¤ºå…ˆæ³¨é‡Šæ‰è¿™äº›ï¼Œåç»­RLå†çœ‹çœ‹æœ‰æ— é—®é¢˜
                    if raw_attention_mask is None: # RL stage get_per_token_logps()
                        # NOTE-ZY: éœ€è¦æ ¹æ®boundaryè°ƒæ•´ï¼Œå¾—åˆ°æ­£ç¡®çš„completion_ids
                        # sys_vis_ids = raw_input_ids[:, :q_tok_start_idx_raw] # system & visual tokens ï¼ˆæ²¡æœ‰åŒ…æ‹¬æœ€åä¸€ä¸ª <|vision_end|>ï¼‰
                        q_completion_ids = raw_input_ids[:, q_tok_start_idx_raw:] # question tokens & completion tokens ï¼ˆè¿™é‡Œé¢æ²¡æœ‰è§†è§‰tokenå³å¯ï¼‰
                        has_img_id = q_completion_ids == self.config.image_token_id # å› ä¸ºRLæœ‰æ—¶å€™ç”Ÿæˆimg or vid id
                        has_vid_id = q_completion_ids == self.config.video_token_id
                        if (has_img_id).any().item():
                            print('Edit tensors ({} image_token_id) for get_rope_index() function!'.format(torch.nonzero(has_img_id).shape[0]))
                            exit(f"å‘ç° get_per_token_logps() ç”Ÿæˆäº† image_token_idï¼Œè¯·æ£€æŸ¥åŸå› ")
                            # q_completion_ids = q_completion_ids.masked_fill(has_img_id, self.config.vision_token_id)
                        if (has_vid_id).any().item():
                            print('Edit tensors ({} video_token_id) for get_rope_index() function!'.format(torch.nonzero(has_vid_id).shape[0]))
                            # q_completion_ids = q_completion_ids.masked_fill(has_vid_id, self.config.vision_token_id)
                            exit(f"å‘ç° get_per_token_logps() ç”Ÿæˆäº† video_token_idï¼Œè¯·æ£€æŸ¥åŸå› ")
                        # temp_input_ids = torch.cat((sys_vis_ids, q_completion_ids), dim=1)
                    
                    # compute position_ids & rope_deltas
                    position_ids_b, rope_deltas_b = self.get_rope_index( # followåŸæœ¬çš„æ“ä½œï¼Œshadowä¸€ä¸‹input 
                        input_ids=temp_input_ids, # e.g., torch.Size([1, 1050])
                        image_grid_thw=image_grid_thw, # e.g., None
                        video_grid_thw=video_grid_thw, # e.g., tensor([[ 8, 16, 30]], device='cuda:0')
                        attention_mask=raw_attention_mask, # e.g., torch.Size([1, 1050])
                    )
                    # import pdb; pdb.set_trace()
                    # NOTE-ZY: checked identical with raw inputs
                    # position_ids_b.shape = torch.Size([3, 1, 1050])ï¼Œæœªè¿›è¡Œmergingçš„pos ids
                    # tensor([[[  0,   1,   2,  ..., 207, 208, 209]],
                    #         [[  0,   1,   2,  ..., 207, 208, 209]],
                    #         [[  0,   1,   2,  ..., 207, 208, 209]]], device='cuda:0')
                    # rope_deltas_b = -840
                    
                    # NOTE: the delta was computed w.r.t. original long sequence and now should be translated w.r.t. compressed sequence
                    # Compressed sequences in a batch have the same length (same cache_position), while raw sequences could have different length (different reduct_cnt & rope_deltas)
                    # Add reduct_cnt to recover raw sequence domain and then apply rope_deltas to cache_position accordingly.
                    rope_deltas_b = rope_deltas_b + reduct_cnt # æ€§èƒ½ååˆ†å—å½±å“ï¼Œå¦‚æœæ²¡å®ç°å¥½çš„è¯ 
                    self.rope_deltas = rope_deltas_b 
                        # NOTE-ZY: rope_deltas_b = -840+720 = -120 ä»ä¸ºè´Ÿæ•°. Pruningçš„æ—¶å€™ä¼šä½¿ç”¨ä¸Šï¼Œå› ä¸ºç”Ÿæˆæ–°tokens -> é‡æ–°ç®—pos ç±»ä¼¼äºpos offset

                    # NOTE-ZY: ä¿®æ”¹æ­£ç¡®çš„pos index
                    # import pdb; pdb.set_trace()
                    # éè§†è§‰token
                    raw_non_visual_mask = (raw_input_ids != self.config.image_token_id) & (raw_input_ids != self.config.video_token_id) # Shape [1, 1050]
                    # è§†è§‰token
                    raw_visual_pos_masks = (raw_input_ids == self.config.image_token_id) | (raw_input_ids == self.config.video_token_id) # Shape [1, 1050]
                    # è·å–é‚£ 960 ä¸ª è§†è§‰ token åœ¨ [1, 1050] åºåˆ—ä¸­çš„ *ç»å¯¹ç´¢å¼•*
                    raw_vis_indices_abs = raw_visual_pos_masks.nonzero(as_tuple=True)[1] # Shape [960], tensor([  11,   12,   13,   14, ..., 1037, 1038, 1039]
                    # token_idx (Shape [240]) åŒ…å«æˆ‘ä»¬æƒ³*ä¿ç•™*çš„ token çš„ *ç›¸å¯¹ç´¢å¼•* (0-959)ï¼Œç”¨å®ƒæ¥ä» 960 ä¸ªç»å¯¹ç´¢å¼•ä¸­é€‰å‡º 240 ä¸ªè¦ä¿ç•™çš„
                    kept_vis_indices_abs = raw_vis_indices_abs[token_idx] # Shape [240], tensor([  14,   18,   22, ..., 1031, 1035, 1039], # è·Ÿtoken_idxå¯¹åº”
                    # åˆ›å»ºä¸€ä¸ª *æ–°* çš„ maskï¼Œåªæ ‡è®°æˆ‘ä»¬*ä¿ç•™*çš„ 240 ä¸ª è§†è§‰ token
                    kept_vis_mask = torch.zeros_like(raw_visual_pos_masks) # Shape [1, 1050], å…¨ False
                    kept_vis_mask[0, kept_vis_indices_abs] = True # å°† 240 ä¸ªä½ç½®è®¾ä¸º True
                    # åˆå¹¶ text_mask å’Œ kept_vis_maskï¼Œ(æ‰€æœ‰æ–‡æœ¬ + ä¿ç•™çš„è§†è§‰) = æœ€ç»ˆçš„ 330 ä¸ª token
                    final_selection_mask = raw_non_visual_mask | kept_vis_mask # Shape [1, 1050]
                    # ä» final_selection_mask ä¸­è·å–æœ€ç»ˆçš„*ç´¢å¼•*
                    final_selection_indices = final_selection_mask.nonzero(as_tuple=True)[1] # Shape [330], tensor([0, 1, ...,8, 9, 10, 14, 18, 22, ...., 1047, 1048, 1049]
                    # ä½¿ç”¨è¿™äº›ç´¢å¼•ä» position_ids_b ä¸­é€‰æ‹©
                    position_ids = position_ids_b[:, :, final_selection_indices] # Shape [3, 1, 330]                    
                    ############# Token Merging with Pre-computed Input IDs #############
            # then use the prev pre-calculated rope-deltas to get the correct position ids
            else:
                batch_size, seq_length, _ = inputs_embeds.shape
                delta = (
                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)
                    if cache_position is not None
                    else 0
                )
                # import pdb; pdb.set_trace()
                position_ids = torch.arange(seq_length, device=inputs_embeds.device)
                position_ids = position_ids.view(1, -1).expand(batch_size, -1)
                if cache_position is not None:  # otherwise `deltas` is an int `0`
                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)
                position_ids = position_ids.add(delta)
                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

        # import pdb; pdb.set_trace() 
        # TODO: check boundaries: [[(14, 17)], 18, 145]
            # boundaries = [[(20, 53), (60, 93), (100, 133), (141, 174), (182, 215), (223, 256), (264, 297), (305, 338)], 339, 435]
            # inputs_embeds.shape = torch.Size([1, 435, 2048])
        
        initial_boundaries = copy.deepcopy(boundaries) # TODO: for rl_forward later use
        
        # NOTE-ZY: qwen2.5vl video-r1ç”¨çš„ self.model(input_ids=None) -> è€Œqwen3vlçš„self.language_model(input_ids=None)
        # print(f"self.language_model()å¼€å¤´ -> input_ids.shape = {input_ids.shape}\nboundaries={boundaries}")
        outputs = self.language_model( # NOTE-ZY: å¯¹åº”video-r1 qwen2_5_vl_video_r1/modeling_qwen2_5_vl.pyçš„self.model()
            input_ids=None,
            position_ids=position_ids, # torch.Size([3, 1, 330]) # NOTE-ZY: å·²ç»æ˜¯token mergingçš„äº†
            attention_mask=attention_mask, # torch.Size([1, 330])ï¼Œå…¨ä¸º1
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds, # torch.Size([1, 330, 4096])
            cache_position=cache_position, # len=330, tensor([  0,   1,   2, ..., 328, 329], device='cuda:0')
            visual_pos_masks=visual_pos_masks, # torch.Size([1, 330])
            deepstack_visual_embeds=deepstack_visual_embeds, # len=3, deepstack_visual_embeds[0].shape = torch.Size([240, 4096]) # NOTE-ZYï¼šå·²ç»è¿›è¡Œäº†token selection
            boundaries=boundaries, # e.g., [[(10, 41), (49, 80), (89, 120), (129, 160), (169, 200), (209, 240), (249, 280), (289, 320)], 321, 330]
            vis_grid_thw=vis_grid_thw, # tensor([[ 8, 16, 30]], device='cuda:0')
            rope_deltas=self.rope_deltas.clone(), # tensor([[-120]], device='cuda:0')
            **kwargs, # {'use_cache': True}
        )

        return Qwen3VLModelOutputWithPast(
            last_hidden_state=outputs.last_hidden_state,
            past_key_values=outputs.past_key_values,
            rope_deltas=self.rope_deltas,
            run_our_forward=run_our_forward,
            is_prefill=is_prefill,
            rl_forward=rl_forward,
            initial_boundaries=initial_boundaries, # TODO: pass initial boundaries for rl_forward later use
            final_boundaries=boundaries,
            input_ids=input_ids,
        )


@dataclass
@auto_docstring(
    custom_intro="""
    Base class for Qwen3VL causal language model (or autoregressive) outputs.
    """
)
class Qwen3VLCausalLMOutputWithPast(ModelOutput):
    r"""
    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
        Language modeling loss (for next-token prediction).
    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
        `past_key_values` input) to speed up sequential decoding.
    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
        The rope index difference between sequence length and multimodal rope.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None
    boundaries: Optional[list[torch.LongTensor]] = None
    updated_input_ids: Optional[torch.LongTensor] = None
    updated_attention_mask: Optional[torch.Tensor] = None
    updated_labels: Optional[torch.LongTensor] = None
    updated_logits: torch.FloatTensor = None
    vision_token_count: Optional[int] = None


class Qwen3VLForConditionalGeneration(Qwen3VLPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {}
    _tied_weights_keys = ["lm_head.weight"]
    # Reference: fix gemma3 grad acc #37208
    accepts_loss_kwargs = False
    config: Qwen3VLConfig

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen3VLModel(config)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)

        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.model.set_decoder(decoder)

    def get_decoder(self):
        return self.model.get_decoder()

    def get_video_features(
        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None
    ):
        return self.model.get_video_features(pixel_values_videos, video_grid_thw)

    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):
        return self.model.get_image_features(pixel_values, image_grid_thw)

    # Make modules available through conditional class for BC
    @property
    def language_model(self):
        return self.model.language_model

    @property
    def visual(self):
        return self.model.visual

    @check_model_inputs
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Qwen3VLCausalLMOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.

        Example:
        """
        
        # import pdb; pdb.set_trace()
        # print(f"Qwen3VLForConditionalGeneration.forward()å¼€å¤´ -> input_ids.shape = {input_ids.shape}")
        
        # NOTE-ZY: qwen2.5vl video-r1ç”¨çš„ self.model(input_ids=None) -> è€Œqwen3vlçš„self.language_model(input_ids=None)
        outputs = self.model( # æŠŠmergeåçš„tokenæ”¾è¿›LLM 1/4 num tokens
            input_ids=input_ids, # e.g., torch.Size([1, 330], num of video_token_id in inputs['input_ids'] = 240
            pixel_values=pixel_values, # e.g., None
            pixel_values_videos=pixel_values_videos, # e.g., torch.Size([3840, 1536])
            image_grid_thw=image_grid_thw, # e.g., None
            video_grid_thw=video_grid_thw, # e.g., tensor([[ 8, 16, 30]]
            position_ids=position_ids, # e.g., None
            attention_mask=attention_mask, # e.g., torch.Size([1, 330]), å…¨ä¸º1çš„matrix
            past_key_values=past_key_values, # e.g., DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer])
            inputs_embeds=inputs_embeds, # e.g., None
            cache_position=cache_position, # e.g., tensor([  0,   1,   2,   3,   4,   ... 329])
            **kwargs,
        )
        # outputs <- return Qwen3VLModelOutputWithPast(
        #     last_hidden_state=outputs.last_hidden_state,
        #     past_key_values=outputs.past_key_values,
        #     rope_deltas=self.rope_deltas,
        #     run_our_forward=run_our_forward,
        #     is_prefill=is_prefill,
        #     rl_forward=rl_forward,
        #     initial_boundaries=initial_boundaries,
        #     final_boundaries=boundaries,
        #     input_ids=input_ids,
        # )

        hidden_states = outputs[0]

        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        loss = None
        updated_labels = None # for SFT
        if labels is not None:
            if outputs.run_our_forward: # é›¶æ•£çš„è¾…åŠ©æ“ä½œï¼Œæ¯”å¦‚modify data type
                ##### NOTE: assume batch as 1 per device during training
                assert labels.shape[0] == 1, f"Batch size > 1 (bs={labels.shape[0]}) æš‚ä¸æ”¯æŒ"
                boundary = outputs.initial_boundaries[0] # [[v_pairs...], q_start, num_tokens]
                vision_boundaries_list = boundary[0] # [[v_start1, v_end1], ...]
                segments_to_keep = []
                last_kept_index = 0
                current_labels = labels[0:1]
                for v_start, v_end in vision_boundaries_list:
                    segments_to_keep.append(current_labels[:, last_kept_index : v_start + 1]) # ä¿ç•™ [last_kept_index : v_start + 1] (ä¿ç•™ <|vision_start|>)
                    last_kept_index = v_end # ä¸¢å¼ƒ (v_start, v_end) ä¹‹é—´, å³ [v_start + 1 : v_end] # ä¸‹ä¸€ä¸ªä¿ç•™çš„èµ·å§‹ç‚¹æ˜¯ v_end (<|vision_end|>)                
                segments_to_keep.append(current_labels[:, last_kept_index:]) # ä¿ç•™æœ€åä¸€ä¸ªç‰‡æ®µ (ä»æœ€åä¸€ä¸ª v_end åˆ°åºåˆ—æœ«å°¾)
                updated_labels = torch.cat(segments_to_keep, dim=-1)
                labels = updated_labels # loss_function å°†ä½¿ç”¨è¿‡æ»¤åçš„ labels
            
            # NOTE: Upcast to float if we need to compute the loss to avoid potential precision issues
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)

        updated_input_ids = None # for RL
        updated_logits = None # for RL; NOTE-ZY: not used
        vision_token_count = 0
        if outputs.run_our_forward:
            if self.training: # NOTE: Upcast to float if we need to compute the loss to avoid potential precision issues (e.g., nan gradients)
                logits = logits.float()

            if outputs.rl_forward: # rl_forward = is_prefill and attention_mask is None
                # RL stage get_per_token_logps() 
                # remove visual tokens
                                
                assert outputs.input_ids.shape[0] == 1, f"Batch size > 1 (bs={outputs.input_ids.shape[0]}) æš‚ä¸æ”¯æŒ"
                # import pdb; pdb.set_trace() # NOTE-ZY: RL forwardç¡®å®èƒ½è¿›æ¥
                boundary = outputs.initial_boundaries # [[v_pairs...], q_start, num_tokens] # TODO: ç”¨initial_boundariesæ¥é€‚é…input_idsçš„vis boundary
                    # TODO: input_ids.shape = torch.Size([1, 473])
                    # outputs.input_ids.shape = torch.Size([1, 473])
                    # torch.equal(input_ids, outputs.input_ids) = True
                    # outputs.boundaries = [[(9, 43), (50, 84), (91, 125), (133, 167), (175, 209), (217, 251), (259, 293), (301, 335)], 336, 473]
                vision_boundaries_list = boundary[0] # [[v_start1, v_end1], ...]
                segments_ids = []
                last_kept_index = 0
                current_input_ids = outputs.input_ids[0:1] # ä¿æŒ batch ç»´åº¦ (1, N)
                for v_start, v_end in vision_boundaries_list:
                    segments_ids.append(current_input_ids[:, last_kept_index : v_start + 1]) # ä¿ç•™ [last_kept_index : v_start + 1] (ä¿ç•™ <|vision_start|>)
                    last_kept_index = v_end # ä¸¢å¼ƒ (v_start, v_end) ä¹‹é—´ # ä¸‹ä¸€ä¸ªä¿ç•™çš„èµ·å§‹ç‚¹æ˜¯ v_end (<|vision_end|>)
                    vision_token_count += (v_end - v_start - 1) # TODO: ç”¨äºåç»­RL codeè®¡ç®—å»æ‰è§†è§‰tokenåçš„prompt_len = prompt_len-vision_token_count
                segments_ids.append(current_input_ids[:, last_kept_index:]) # ä¿ç•™æœ€åä¸€ä¸ªç‰‡æ®µ
                updated_input_ids = torch.cat(segments_ids, dim=-1)        
                
                # final_boundary = outputs.final_boundaries # [[v_pairs...], q_start, num_tokens]
                # for v_start, v_end in final_boundary[0]:
                     # count number of vision tokens
                
                # print(f"enter rl_forward (rl_forward = is_prefill and attention_mask is None) for RL stage get_per_token_logps()")
                # print(f"outputs.initial_boundaries = {outputs.initial_boundaries}")
                # print(f"outputs.final_boundaries = {outputs.final_boundaries}")
                # print(f"input_ids.shape = {input_ids.shape}")
                # print(f"outputs.input_ids.shape = {outputs.input_ids.shape}")
                # print(f"updated_input_ids.shape = {updated_input_ids.shape}")
                # print(f"logits.shape = {logits.shape}")
                # enter rl_forward (rl_forward = is_prefill and attention_mask is None) for RL stage get_per_token_logps()
                # outputs.boundaries = [[(9, 43), (50, 84), (91, 125), (133, 167), (175, 209), (217, 251), (259, 293), (301, 335)], 336, 473]
                # input_ids.shape = torch.Size([1, 473])
                # outputs.input_ids.shape = torch.Size([1, 473])
                # updated_input_ids.shape = torch.Size([1, 209])
                # logits.shape = torch.Size([1, 209, 151936])
                # import pdb;pdb.set_trace() # TODO: è¿˜æ˜¯ç”¨video debugæ›´æ–¹ä¾¿checké€»è¾‘

                # TODO: returnçš„boundaryåº”è¯¥æ˜¯æœ€ç»ˆçš„boundaryï¼Ÿ

        nan_exist = torch.isnan(logits).any().item()
        if nan_exist:
            if outputs.is_prefill:
                if outputs.rl_forward:
                    print("\nRL stage: \n", nan_exist)
                    import pdb; pdb.set_trace()
                else:
                    print("\nprefill stage: \n", nan_exist)
                    import pdb; pdb.set_trace()
            else:
                print("\ndecode stage: \n", nan_exist)
                import pdb; pdb.set_trace()

        return Qwen3VLCausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            rope_deltas=outputs.rope_deltas,
            boundaries=outputs.final_boundaries, # TODO: pass final boundaries
            updated_labels=updated_labels, # for SFT
            updated_input_ids=updated_input_ids, # for RL
            updated_logits=updated_logits, # for RL
            vision_token_count=vision_token_count, # for RL
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        use_cache=True,
        pixel_values=None,
        pixel_values_videos=None,
        image_grid_thw=None,
        video_grid_thw=None,
        **kwargs,
    ):
        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model

        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            position_ids=position_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            use_cache=use_cache,
            **kwargs,
        )

        # Qwen3VL position_ids are prepareed with rope_deltas in forward
        model_inputs["position_ids"] = None

        if cache_position[0] != 0:
            model_inputs["pixel_values"] = None
            model_inputs["pixel_values_videos"] = None

        return model_inputs

    def _get_image_nums_and_video_nums(
        self,
        input_ids: Optional[torch.LongTensor],
        inputs_embeds: Optional[torch.Tensor] = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.
        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.

        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary.

        Returns:
            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)
            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)
        """
        image_token_id = self.config.image_token_id
        video_token_id = self.config.video_token_id
        vision_start_token_id = self.config.vision_start_token_id

        if inputs_embeds is not None:
            vision_start_mask = (
                inputs_embeds
                == self.get_input_embeddings()(
                    torch.tensor(vision_start_token_id, dtype=torch.long, device=inputs_embeds.device)
                )
            )[..., 0]
            image_mask = (
                inputs_embeds
                == self.get_input_embeddings()(
                    torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)
                )
            )[..., 0]
            video_mask = (
                inputs_embeds
                == self.get_input_embeddings()(
                    torch.tensor(video_token_id, dtype=torch.long, device=inputs_embeds.device)
                )
            )[..., 0]
        else:
            vision_start_mask = input_ids == vision_start_token_id
            image_mask = input_ids == image_token_id
            video_mask = input_ids == video_token_id

        vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)
        image_nums = torch.sum(vision_first_mask & image_mask, dim=1)
        video_nums = torch.sum(vision_first_mask & video_mask, dim=1)

        return image_nums, video_nums

    def _expand_inputs_for_generation(
        self,
        expand_size: int = 1,
        is_encoder_decoder: bool = False,
        input_ids: Optional[torch.LongTensor] = None,
        **model_kwargs,
    ) -> tuple[torch.LongTensor, dict[str, Any]]:
        # Overwritten -- Support for expanding tensors without a batch size dimension
        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t
        # pixel_values.shape[0] is sum(seqlen_images for samples)
        # image_grid_thw.shape[0] is sum(num_images for samples)

        if expand_size == 1:
            return input_ids, model_kwargs

        visual_keys = ["pixel_values", "image_grid_thw", "pixel_values_videos", "video_grid_thw", "second_per_grid_ts"]

        def _expand_dict_for_generation_visual(dict_to_expand):
            image_grid_thw = model_kwargs.get("image_grid_thw", None)
            video_grid_thw = model_kwargs.get("video_grid_thw", None)
            
            ###### NOTE-ZY: fix qwen3vl use num_return_sequences > 1, refer to https://github.com/QwenLM/Qwen3-VL/issues/1621
            # flatten video grid thw to 1D tensor
            if video_grid_thw is not None:
                video_grid_thw = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)
                video_grid_thw[:, 0] = 1
            ###### 
            
            image_nums, video_nums = self._get_image_nums_and_video_nums(
                input_ids, inputs_embeds=model_kwargs.get("inputs_embeds", None)
            )

            def _repeat_interleave_samples(x, lengths, repeat_times):
                samples = torch.split(x, lengths)
                repeat_args = [repeat_times] + [1] * (x.dim() - 1)
                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)
                return result

            for key in dict_to_expand:
                if key == "pixel_values":
                    # split images into samples
                    samples = torch.split(image_grid_thw, list(image_nums))
                    # compute the sequence length of images for each sample
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "image_grid_thw":
                    # get the num of images for each sample
                    lengths = list(image_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "pixel_values_videos":
                    samples = torch.split(video_grid_thw, list(video_nums))
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "video_grid_thw":
                    lengths = list(video_nums)
                    
                    ###### NOTE-ZY: fix qwen3vl use num_return_sequences > 1, refer to https://github.com/QwenLM/Qwen3-VL/issues/1621
                    dict_to_expand[key] = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)
                    dict_to_expand[key][:, 0] = 1
                    ######
                    
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "second_per_grid_ts":
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=list(video_nums), repeat_times=expand_size
                    )
            return dict_to_expand

        def _expand_dict_for_generation(dict_to_expand):
            for key in dict_to_expand:
                if (
                    key != "cache_position"
                    and dict_to_expand[key] is not None
                    and isinstance(dict_to_expand[key], torch.Tensor)
                    and key not in visual_keys
                ):
                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)
            return dict_to_expand

        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)

        if input_ids is not None:
            input_ids = input_ids.repeat_interleave(expand_size, dim=0)

        model_kwargs = _expand_dict_for_generation(model_kwargs)

        if is_encoder_decoder:
            if model_kwargs.get("encoder_outputs") is None:
                raise ValueError("If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.")
            model_kwargs["encoder_outputs"] = _expand_dict_for_generation(model_kwargs["encoder_outputs"])

        return input_ids, model_kwargs


__all__ = [
    "Qwen3VLVisionModel",
    "Qwen3VLForConditionalGeneration",
    "Qwen3VLModel",
    "Qwen3VLPreTrainedModel",
    "Qwen3VLTextModel",
]


################################################################################################
################################################################################################
################################################################################################
def get_protected_info(boundaries, hidden_states):
    """ 
    [MODIFIED]
    Given the boundary (as a Python list) of different types of tokens in a sequence, 
    this function outputs 2D indices (B, N) to protect all *non-visual* (text) tokens.

    'boundaries' is assumed to be a list of 'boundaries_b' items (one item per batch element).
    Each 'boundaries_b' is [vision_start_end_idx_pairs, q_start_idx, num_tokens]
    
    protected_inds: all indices in a batch.
    protected_num: number of tokens per sample in a batch.
    """    
    # the indices of text tokens that should be protected
    protected_inds = []
    protected_num = []
    
    # 'boundaries' is now a Python list, e.g., [vision_start_end_idx_pairs, q_start_idx, num_tokens]
    this_bound = boundaries
    vision_start_end_idx_pairs = this_bound[0] # e.g., [(10, 41), (49, 80), ...], ]
    num_tokens = this_bound[2]                 # e.g., 330 (== hidden_states[0].size(0))
    assert num_tokens == hidden_states[0].size(0)
    
    this_inds_col_chunks = []
    current_idx = 0
    # Loop through all visual chunks
    # import pdb; pdb.set_trace()
    for (start_idx, end_idx) in vision_start_end_idx_pairs:
        # 1. [MODIFIED] æ·»åŠ æ­¤ visual chunk ä¹‹å‰çš„ æ–‡æœ¬/ç³»ç»Ÿ token
        if current_idx < start_idx:
            text_chunk = torch.arange(current_idx, start_idx, device=hidden_states.device)
            this_inds_col_chunks.append(text_chunk)
        
        # 2. [NEW] æ·»åŠ  <|vision_start|> token (å®ƒä¹Ÿæ˜¯éè§†è§‰ token)
        start_token_chunk = torch.arange(start_idx, start_idx + 1, device=hidden_states.device)
        this_inds_col_chunks.append(start_token_chunk)

        # 3. [NEW] æ·»åŠ  <|vision_end|> token (å®ƒä¹Ÿæ˜¯éè§†è§‰ token)
        #    (è·³è¿‡ [start_idx + 1, end_idx - 1] ä¹‹é—´çš„è§†è§‰ pad token)
        end_token_chunk = torch.arange(end_idx, end_idx + 1, device=hidden_states.device)
        this_inds_col_chunks.append(end_token_chunk)

        # 4. æ›´æ–° current_idxï¼Œè·³è¿‡è¿™ä¸ª visual chunk
        current_idx = end_idx + 1
    
    # 5. æ·»åŠ æœ€åä¸€ä¸ª visual chunk ä¹‹åçš„ æ–‡æœ¬ (query) token
    if current_idx < num_tokens:
        final_text_chunk = torch.arange(current_idx, num_tokens, device=hidden_states.device)
        this_inds_col_chunks.append(final_text_chunk)    
    # --- Original logic continues below ---
    
    # å°†æ‰€æœ‰æ–‡æœ¬å—ï¼ˆsynã€vstartã€vendã€text padã€Queryï¼‰æ‹¼æ¥åœ¨ä¸€èµ·
    this_inds_col = torch.cat(this_inds_col_chunks).unsqueeze(1) 
        # torch.Size([90, 1]), tensor([[  0], [  1], ... [ 10], [ 41],  [329]], device='cuda:0') 
        # å¯¹åº”vision_start_end_idx_pairsï¼Œä¸åŒ…æ‹¬visual token
        # 90 = 330-30*8
    
    # æ·»åŠ  batch index (ç¡®ä¿åœ¨åŒä¸€è®¾å¤‡ä¸Š)
    this_inds_row = torch.ones(this_inds_col.shape[0], device=hidden_states.device).fill_(0).long().unsqueeze(1) # tensor([[0], [0], ...]

    this_inds = torch.cat((this_inds_row, this_inds_col), dim=1) 
        # torch.Size([90, 2]), tensor([[0,  0], [0,  1], ... [0, 10], [0, 41],  [0,329]], device='cuda:0') 
     
    # import pdb; pdb.set_trace()
     
    protected_inds = this_inds
    protected_num = torch.tensor([this_inds.shape[0]], dtype=protected_inds.dtype, device=protected_inds.device) # tensor([90], device='cuda:0')

    return protected_inds, protected_num

def get_importance(attention_score, tau_imp=0.1, iters=10, d=0.):
    """ Given attention logits (before softmax & already processed by casual mask), run page rank algorithm to compute importance scores.
    """
    iters = 2
    attn = attention_score.to(torch.float32) # attention_score.clone().detach().to(torch.float32)
    B,H,N,_ = attn.shape  
    M = nn.functional.softmax(attn / tau_imp, dim=-1, dtype=torch.float32) # .to(attn.dtype) # softmax with a temperature

    # page rank
    dist = torch.ones(B,H,1,N).to(M.device, M.dtype) / N
    for i in range(iters):
        dist = (dist@M)*(1-d) + d/N   ## smoothing the iterations
    
    # square to magnify importance
    dist = torch.mean(dist,dim=1) # torch.mean(dist.pow(2),dim=1).pow(0.5)

    # return score
    importance = dist.view(B, N) 
    return importance

def bipartite_soft_matching_merge(
    metric: torch.Tensor,
    r: int,
    x: torch.Tensor,
    mode: str = "mean",
    token_idx: torch.Tensor = None,
) -> torch.Tensor:
    """
    Modified from the implementation of paper https://arxiv.org/abs/2210.09461
    Batch token merging with a balanced matching set (50%, 50%).

    Input size is [batch, tokens, channels].
    r indicates the number of tokens to remove (max 50% of tokens).
    """
    protected = 0

    # We can only reduce by a maximum of 50% tokens
    t = metric.shape[1]
    r = min(r, (t - protected) // 2)

    if r <= 0:
        return None

    with torch.no_grad():
        metric = metric / metric.norm(dim=-1, keepdim=True)
        a, b = metric[..., ::2, :], metric[..., 1::2, :] # a: source, b: dst
        scores = a @ b.transpose(-1, -2) # row: source, col: dst

        node_max, node_idx = scores.max(dim=-1) # col index (dst nodes): for each node (row), find a best matched node (col)
        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None] # row index (source nodes): rank best-matched pairs
        ################################# maintain relative order of unmerged tokens #################################
        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens, the source nodes that do not related to merging operation (low similarity part)
        unm_idx, _ = unm_idx.sort(dim=-2, descending=False)
        ################################# maintain relative order of unmerged tokens #################################
        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens, the source nodes that do not related to merging operation (low similarity part)
        src_idx = edge_idx[..., :r, :]  # Merged Tokens, the source nodes to be merged (high similarity part)
        dst_idx = node_idx[..., None].gather(dim=-2, index=src_idx) # the selected values in node_idx are the dst nodes during merging operation, the values can be duplicated

        # merge tokens
        src, dst = x[..., ::2, :], x[..., 1::2, :]
        n, t1, c = src.shape
        unm = src.gather(dim=-2, index=unm_idx.expand(n, t1 - r, c)) # the source nodes that do not related to merging operation 
        src = src.gather(dim=-2, index=src_idx.expand(n, r, c)) # the source nodes to be merged
        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode) # add source nodes to the indices in dst_idx, and merge them to the nodes in dst via 'mode'
        
        # final results
        merged_feats = torch.cat((unm, dst), dim=1) # [the source nodes that do not relate to merging, the dst nodes that already merged with matched source nodes], first item was sorted 
        if token_idx is not None:
            src_token_idx, dst_token_idx = token_idx[..., ::2, :], token_idx[..., 1::2, :]
            unm_token_idx = src_token_idx.gather(dim=-2, index=unm_idx.expand(n, t1 - r, 1))
            merged_token_idx = torch.cat((unm_token_idx, dst_token_idx), dim=1)
            return merged_feats, merged_token_idx
        else:
            return merged_feats